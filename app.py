import streamlit as st
import pandas as pd
import json
import warnings
import os
import time
import re
import jieba
import random
import concurrent.futures
import math
import threading
from google import genai
from google.genai import types

# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings('ignore')

# ================= 1. åŸºç¡€é…ç½® =================

st.set_page_config(
    page_title="ChatMDM - åœ°åŒºèšåˆBatchç‰ˆ", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

# --- æ¨¡å‹é…ç½® (ä¿æŒä¸å˜) ---
MODEL_NAME = "gemini-3-pro-preview" 

# --- å…¨å±€å¸¸é‡ ---
MASTER_COL_NAME = "åŒ»é™¢åç§°"
MASTER_COL_CODE = "åŒ»é™¢ç¼–ç "
MASTER_COL_PROV = "çœä»½"
MASTER_COL_CITY = "åŸå¸‚"
CACHE_FILE = "mdm_cache.pkl"

BATCH_SIZE = 20       # æ¯æ‰¹å¤„ç†å¤šå°‘æ¡å¾…æ¸…æ´—æ•°æ®
CANDIDATE_LIMIT = 500 # å€™é€‰æ± æœ€å¤§å®¹é‡ (å»ºè®®: å¦‚æœä¾ç„¶é¢‘å‘429ï¼Œå¯é…Œæƒ…é™è‡³200)
MAX_RETRIES = 3       # åŸºç¡€é‡è¯•æ¬¡æ•° (é’ˆå¯¹é429é”™è¯¯)

# --- API Key è§£æ ---
try:
    keys_str = st.secrets.get("GENAI_API_KEY", os.getenv("GENAI_API_KEY", ""))
    API_KEYS = [k.strip() for k in keys_str.split(',') if k.strip()]
    if not API_KEYS:
        API_KEYS = [""]
except:
    API_KEYS = [""]

# ================= 2. è§†è§‰ä½“ç³» =================

def inject_custom_css():
    st.markdown("""
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp { background-color: #050505; background-image: radial-gradient(circle at 50% 0%, #1a1a2e 0%, #050505 40%); font-family: 'Inter', sans-serif; }
        .glass-card { background: rgba(255, 255, 255, 0.03); backdrop-filter: blur(10px); border: 1px solid rgba(255, 255, 255, 0.08); border-radius: 12px; padding: 20px; margin-bottom: 20px; }
        .metric-label { font-size: 12px; color: #94a3b8; text-transform: uppercase; letter-spacing: 1px; }
        .metric-value { font-size: 28px; font-weight: 700; color: #ffffff; }
        .metric-sub { font-size: 12px; color: #64748b; margin-top: 4px; }
        [data-testid="stSidebar"] { background-color: #000000 !important; border-right: 1px solid #222; }
        [data-testid="stDataFrame"] { border: 1px solid #333; border-radius: 8px; }
        .stProgress > div > div > div > div { background-color: #3b82f6; }
        </style>
    """, unsafe_allow_html=True)

def render_metric_card(label, value, sub_text=""):
    st.markdown(f"""<div class="glass-card"><div class="metric-label">{label}</div><div class="metric-value">{value}</div><div class="metric-sub">{sub_text}</div></div>""", unsafe_allow_html=True)

# ================= 3. NLP & æ•°æ®å·¥å…· =================

STOP_WORDS = {
    "åŒ»é™¢", "æœ‰é™å…¬å¸", "æœ‰é™", "è´£ä»»", "å…¬å¸", "åˆ†é™¢", "é™„å±", 
    "å­¦", "æ ¡", "å«ç”Ÿ", "æœåŠ¡", "ä¸­å¿ƒ", "ç«™", "æ‰€", "é—¨è¯Š", "éƒ¨",
    "çœ", "å¸‚", "åŒº", "å¿", "è¡—é“", "ç¤¾åŒº"
}

def extract_core_tokens(text):
    if not isinstance(text, str): return set()
    text = re.sub(r'[ï¼ˆ(].*?[)ï¼‰]', '', text)
    words = jieba.lcut_for_search(text)
    tokens = set()
    for w in words:
        w = w.strip()
        if w not in STOP_WORDS and len(w) > 1:
            tokens.add(w)
    return tokens

# --- æ”¹è¿›ï¼šAPI Key ç®¡ç†å™¨ (è§£å†³å¹¶å‘å†²çª) ---
class KeyManager:
    def __init__(self, api_keys):
        self.clients = []
        # åˆå§‹åŒ–æ‰€æœ‰æœ‰æ•ˆçš„ Client
        for k in api_keys:
            if k:
                try:
                    self.clients.append(genai.Client(api_key=k, http_options={'api_version': 'v1beta'}))
                except:
                    pass
        self.num_keys = len(self.clients)
        self.current_idx = 0
        self._lock = threading.Lock() # çº¿ç¨‹é”

    def get_next_client(self):
        if self.num_keys == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„ API Key")
        
        with self._lock:
            client = self.clients[self.current_idx]
            self.current_idx = (self.current_idx + 1) % self.num_keys
        return client

@st.cache_resource
def get_key_manager():
    return KeyManager(API_KEYS)

def load_cached_master():
    if os.path.exists(CACHE_FILE):
        try: return pd.read_pickle(CACHE_FILE)
        except: return None
    return None

def save_master_cache(df):
    df.to_pickle(CACHE_FILE)

def clear_master_cache():
    if os.path.exists(CACHE_FILE): os.remove(CACHE_FILE)

def process_master_data(uploaded_file):
    try:
        if uploaded_file.name.endswith('.xlsx'): df = pd.read_excel(uploaded_file, engine='openpyxl')
        else: df = pd.read_csv(uploaded_file)
        df = df.astype(str)
        df.columns = df.columns.str.strip()
        for col in df.columns: df[col] = df[col].apply(lambda x: x.strip().replace('nan', '') if x != 'nan' else '')
        
        col_map_rename = {}
        for col in df.columns:
            if "åç§°" in col and "åŒ»é™¢" in col: col_map_rename[col] = MASTER_COL_NAME
            elif "ç¼–ç " in col: col_map_rename[col] = MASTER_COL_CODE
            elif "çœ" in col: col_map_rename[col] = MASTER_COL_PROV
            elif "å¸‚" in col: col_map_rename[col] = MASTER_COL_CITY
        if col_map_rename: df = df.rename(columns=col_map_rename)
        
        with st.spinner("æ­£åœ¨æ„å»ºæœç´¢å¼•æ“ç´¢å¼•..."):
            df['tokens'] = df[MASTER_COL_NAME].apply(extract_core_tokens)
        return df, "SUCCESS"
    except Exception as e: return None, str(e)

def clean_json_response(text):
    if not text: return None
    text = text.strip()
    try: return json.loads(text)
    except: pass
    try:
        pattern = r"```(?:json)?\s*(.*?)\s*```"
        match = re.search(pattern, text, re.DOTALL)
        if match: return json.loads(match.group(1))
    except: pass
    try:
        start = text.find('[')
        end = text.rfind(']')
        if start != -1 and end != -1: return json.loads(text[start:end+1])
    except: pass
    return None

# ================= 4. æ‰¹é‡æ™ºèƒ½åŒ¹é…é€»è¾‘ (Batch Logic) =================

def get_batch_candidates(df_master, target_batch_df, col_map, limit=500):
    first_row = target_batch_df.iloc[0]
    t_prov = str(first_row.get(col_map['target_province'], ''))
    t_city = str(first_row.get(col_map['target_city'], ''))
    
    candidates = pd.DataFrame()
    if t_city and len(t_city) > 1 and t_city != 'nan' and t_city != 'æ— ':
        candidates = df_master[df_master[MASTER_COL_CITY] == t_city].copy()
    
    if candidates.empty and t_prov and len(t_prov) > 1 and t_prov != 'nan':
        candidates = df_master[df_master[MASTER_COL_PROV] == t_prov].copy()
        
    if candidates.empty:
        return pd.DataFrame(), "æ— åœ°åŒºåŒ¹é…"

    if len(candidates) > limit:
        batch_tokens = set()
        for val in target_batch_df[col_map['target_name']]:
            batch_tokens.update(extract_core_tokens(str(val)))
        
        def calc_batch_overlap(master_tokens):
            if not master_tokens: return 0
            return len(batch_tokens & master_tokens)
            
        candidates['overlap'] = candidates['tokens'].apply(calc_batch_overlap)
        candidates = candidates.sort_values('overlap', ascending=False).head(limit)
        
    return candidates, f"åŒºåŸŸ:{t_prov}-{t_city}"

def call_ai_batch_process(key_manager, target_batch_df, candidates_df, col_map, batch_id):
    """
    Batch API è°ƒç”¨ - åŒ…å« 429 é”™è¯¯é‡è¯•ä¸ Key è½®è¯¢æœºåˆ¶
    """
    cand_str_list = []
    cand_map = {}
    for _, row in candidates_df.iterrows():
        rid = str(row[MASTER_COL_CODE]).strip()
        name = row[MASTER_COL_NAME]
        cand_str_list.append(f"ID:{rid} | {name}")
        cand_map[rid] = row.to_dict()
    
    candidates_text = "\n".join(cand_str_list)

    targets_list = []
    for idx, row in target_batch_df.iterrows():
        t_name = str(row[col_map['target_name']])
        targets_list.append(f"TaskID:{str(idx)} | å¾…æ´—åç§°: {t_name}")
    
    targets_text = "\n".join(targets_list)

    # --- ä½ çš„åŸå§‹æç¤ºè¯ (ä¿æŒä¸å˜) ---
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®æ¸…æ´—åŠ©æ‰‹ã€‚è¯·å°†ã€å¾…æ¸…æ´—åˆ—è¡¨ã€‘ä¸­çš„æœºæ„åç§°ï¼ŒåŒ¹é…åˆ°ã€æ ‡å‡†å€™é€‰æ± ã€‘ä¸­å”¯ä¸€çš„æœºæ„ã€‚
    
    ã€æ ‡å‡†å€™é€‰æ± ã€‘(ä»…é™ä»æ­¤åˆ—è¡¨ä¸­é€‰æ‹©):
    {candidates_text}
    
    ã€å¾…æ¸…æ´—åˆ—è¡¨ã€‘:
    {targets_text}
    
    ã€è¦æ±‚ã€‘:
    1. è¿”å›ä¸€ä¸ªJSONåˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰ TaskID çš„ç»“æœã€‚
    2. å¦‚æœåç§°é«˜åº¦ç›¸ä¼¼ï¼Œåˆ™è§†ä¸ºåŒ¹é…ã€‚
    3. å¦‚æœåœ¨å€™é€‰æ± ä¸­æ‰¾ä¸åˆ°åŒ¹é…é¡¹ï¼Œmatched_id ä¸º nullã€‚
    4. å³ä½¿å®Œå…¨ä¸åŒ¹é…ï¼Œä¹Ÿè¦è¿”å›è¯¥ TaskIDã€‚
    
    ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ã€‘:
    [
        {{"task_id": "12", "matched_id": "CODE001", "confidence": 0.95, "reason": "å…¨åä¸€è‡´"}},
        {{"task_id": "13", "matched_id": null, "confidence": 0.0, "reason": "æ— ç›¸ä¼¼é¡¹"}}
    ]
    """

    last_error = ""
    last_raw_resp = ""
    
    # é’ˆå¯¹ 429 é”™è¯¯çš„é‡è¯•æ¬¡æ•°
    RETRIES_FOR_429 = 6 
    
    for attempt in range(RETRIES_FOR_429):
        try:
            # ä½¿ç”¨ Key Manager è·å–ä¸‹ä¸€ä¸ª Key
            client = key_manager.get_next_client()
            
            # ä¸ºäº†é˜²æ­¢ç¬é—´å¹¶å‘è¿‡é«˜ï¼ŒåŠ å…¥å¾®å°çš„éšæœºç­‰å¾…
            time.sleep(random.uniform(0.1, 0.3))

            response = client.models.generate_content(
                model=MODEL_NAME, # ä¿æŒä½ çš„æ¨¡å‹é€‰æ‹©
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            
            last_raw_resp = response.text
            result_list = clean_json_response(response.text)
            
            if isinstance(result_list, list) and len(result_list) > 0:
                parsed_results = []
                for res in result_list:
                    raw_task_id = res.get('task_id')
                    if raw_task_id is None: continue
                    task_id_str = str(raw_task_id)

                    matched_id = res.get('matched_id')
                    out_row = {
                        "idx_key": task_id_str,
                        "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                        "æ ‡å‡†ç¼–ç ": None, "æ ‡å‡†åç§°": None, 
                        "æ ‡å‡†çœä»½": None, "æ ‡å‡†åŸå¸‚": None,
                        "ç½®ä¿¡åº¦": res.get('confidence', 0.0),
                        "åŒ¹é…åŸå› ": res.get('reason', 'AIæœªæ‰¾åˆ°')
                    }
                    
                    if matched_id:
                        matched_id_str = str(matched_id).strip()
                        if matched_id_str in cand_map:
                            m_row = cand_map[matched_id_str]
                            out_row.update({
                                "åŒ¹é…çŠ¶æ€": "AIåŒ¹é…",
                                "æ ‡å‡†ç¼–ç ": m_row[MASTER_COL_CODE],
                                "æ ‡å‡†åç§°": m_row[MASTER_COL_NAME],
                                "æ ‡å‡†çœä»½": m_row[MASTER_COL_PROV],
                                "æ ‡å‡†åŸå¸‚": m_row[MASTER_COL_CITY]
                            })
                    parsed_results.append(out_row)
                return parsed_results
            
            else:
                # ç»“æœä¸ºç©ºï¼Œè§†ä¸ºå¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸è§¦å‘é‡è¯•ï¼ˆé™¤éæ˜¯æœ€åä¸€æ¬¡ï¼‰
                raise ValueError("Empty or Invalid JSON response")

        except Exception as e:
            last_error = str(e)
            
            # æ ¸å¿ƒï¼šå¤„ç† 429 Too Many Requests
            if "429" in last_error or "503" in last_error or "Resource exhausted" in last_error:
                # æŒ‡æ•°é€€é¿: 2s -> 4s -> 8s -> 16s ...
                sleep_time = (2 ** attempt) + random.uniform(1, 3)
                print(f"âš ï¸ è§¦å‘é™æµ ({last_error[:20]}...), çº¿ç¨‹ä¼‘çœ  {sleep_time:.1f}s åé‡è¯•...")
                time.sleep(sleep_time)
                continue # ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯
            else:
                # å…¶ä»–é”™è¯¯ï¼ˆå¦‚ 400 Bad Requestï¼‰é€šå¸¸ä¸å¯é‡è¯•ï¼Œæˆ–åªé‡è¯• 1-2 æ¬¡
                if attempt < 2: 
                    time.sleep(2)
                    continue
                break
                
    return [{"error": f"{last_error} | RAW: {last_raw_resp[:50]}"}]

def process_batch_job(batch_data, df_master, col_map, key_manager):
    """
    Worker å‡½æ•°ï¼šå¤„ç†ä¸€ä¸ª Batch
    """
    (prov, city), df_batch = batch_data
    
    candidates, source_info = get_batch_candidates(df_master, df_batch, col_map, limit=CANDIDATE_LIMIT)
    
    results = []
    
    if candidates.empty:
        for idx, _ in df_batch.iterrows():
            results.append({
                "idx": idx,
                "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                "åŒ¹é…åŸå› ": f"æ ‡å‡†åº“ä¸­æ— [{prov}-{city}]æ•°æ®",
                "ç½®ä¿¡åº¦": 0.0
            })
        return results

    # è°ƒç”¨ AIï¼Œä¼ å…¥ key_manager
    ai_results = call_ai_batch_process(key_manager, df_batch, candidates, col_map, f"{prov}_{city}")
    
    if len(ai_results) == 1 and "error" in ai_results[0]:
        err_msg = ai_results[0]["error"]
        for idx, _ in df_batch.iterrows():
            results.append({
                "idx": idx,
                "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                "åŒ¹é…åŸå› ": f"APIè°ƒç”¨å¤±è´¥: {err_msg}",
                "ç½®ä¿¡åº¦": 0.0
            })
        return results

    ai_res_map = {r['idx_key']: r for r in ai_results if 'idx_key' in r}
    
    final_results = []
    for idx, _ in df_batch.iterrows():
        idx_str = str(idx) 
        
        if idx_str in ai_res_map:
            res_data = ai_res_map[idx_str].copy()
            del res_data['idx_key']
            res_data['idx'] = idx
            final_results.append(res_data)
        else:
            final_results.append({
                "idx": idx,
                "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                "åŒ¹é…åŸå› ": "AIå“åº”é—æ¼(IDæœªè¿”å›)",
                "ç½®ä¿¡åº¦": 0.0
            })
            
    return final_results

# ================= 5. UI ä¸ ä¸»é€»è¾‘ =================

inject_custom_css()
key_manager = get_key_manager() # è·å– Key ç®¡ç†å™¨

if "df_result" not in st.session_state: st.session_state.df_result = None
if "mapping_confirmed" not in st.session_state: st.session_state.mapping_confirmed = False
if "processing" not in st.session_state: st.session_state.processing = False
if "stop_signal" not in st.session_state: st.session_state.stop_signal = False
if "col_map" not in st.session_state: st.session_state.col_map = {}
if "df_master" not in st.session_state: st.session_state.df_master = load_cached_master()

with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/3063/3063823.png", width=60)
    st.title("ChatMDM")
    st.caption("Region-Batch Edition")
    st.markdown("---")
    
    if st.session_state.df_master is not None:
        st.success(f"âœ… æ ‡å‡†åº“: {len(st.session_state.df_master):,} æ¡")
        if st.button("ğŸ—‘ï¸ é‡æ–°ä¸Šä¼ æ ‡å‡†åº“"):
            clear_master_cache()
            st.session_state.df_master = None
            st.rerun()
    else:
        master_file = st.file_uploader("ä¸Šä¼ æ ‡å‡†åº“ (xlsx/csv)", type=["xlsx", "csv"])
        if master_file:
            df_proc, msg = process_master_data(master_file)
            if df_proc is not None:
                st.session_state.df_master = df_proc
                save_master_cache(df_proc)
                st.rerun()
            else: st.error(msg)
    
    st.divider()
    if st.button("ğŸ”„ é‡ç½®æ‰€æœ‰ä»»åŠ¡"):
        bak = st.session_state.df_master
        st.session_state.clear()
        st.session_state.df_master = bak
        st.rerun()
        
    if st.session_state.df_result is not None:
        st.divider()
        df_exp = st.session_state.df_result
        csv = df_exp.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ", csv, "mdm_result.csv", "text/csv", type="primary")

st.title("ğŸ¥ åŒ»ç–—ä¸»æ•°æ®æ¸…æ´— (åœ°åŒºèšåˆ + Batchå¹¶å‘)")

if key_manager.num_keys == 0: st.error("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·åœ¨ Secrets ä¸­é…ç½® GENAI_API_KEY")
if st.session_state.df_master is None: st.info("è¯·å…ˆä¸Šä¼ æ ‡å‡†åº“"); st.stop()

# 1. ä¸Šä¼ å¾…æ´—æ•°æ®
if st.session_state.df_result is None:
    target_file = st.file_uploader("ä¸Šä¼ å¾…æ¸…æ´—æ•°æ®", type=["xlsx", "csv"])
    if target_file:
        if target_file.name.endswith('.csv'): df_t = pd.read_csv(target_file)
        else: df_t = pd.read_excel(target_file)
        df_t = df_t.astype(str)
        for c in ['åŒ¹é…çŠ¶æ€', 'æ ‡å‡†ç¼–ç ', 'æ ‡å‡†åç§°', 'æ ‡å‡†çœä»½', 'æ ‡å‡†åŸå¸‚', 'åŒ¹é…åŸå› ']: df_t[c] = None
        df_t['åŒ¹é…çŠ¶æ€'] = 'å¾…å¤„ç†'
        df_t['ç½®ä¿¡åº¦'] = 0.0
        st.session_state.df_result = df_t
        st.rerun()

# 2. æ˜ å°„å­—æ®µ
elif not st.session_state.mapping_confirmed:
    cols = st.session_state.df_result.columns.tolist()
    c1, c2, c3 = st.columns(3)
    t_name = c1.selectbox("åç§°åˆ—", cols)
    t_prov = c2.selectbox("çœä»½åˆ—", cols)
    t_city = c3.selectbox("åŸå¸‚åˆ—", cols)
    
    if st.button("ğŸš€ å¼€å§‹æ¸…æ´—é…ç½®"):
        st.session_state.col_map = {"target_name": t_name, "target_province": t_prov, "target_city": t_city}
        st.session_state.mapping_confirmed = True
        st.rerun()

# 3. æ‰§è¡Œæ§åˆ¶å°
else:
    df_curr = st.session_state.df_result
    col_map = st.session_state.col_map
    
    done = len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] != 'å¾…å¤„ç†'])
    c1, c2, c3, c4 = st.columns(4)
    render_metric_card("æ€»è¿›åº¦", f"{done}/{len(df_curr)}")
    render_metric_card("å…¨å­—åŒ¹é…", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'å…¨å­—åŒ¹é…']))
    render_metric_card("AI å‘½ä¸­", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'AIåŒ¹é…']))
    render_metric_card("æœªå‘½ä¸­", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'AIæœªåŒ¹é…']))
    
    st.divider()
    
    col_act, col_view = st.columns([1, 4])
    
    with col_act:
        if st.button("âš¡ Step 1: ç²¾ç¡®åŒ¹é…", use_container_width=True, disabled=st.session_state.processing):
            with st.spinner("Hash ç¢°æ’ä¸­..."):
                master_deduped = st.session_state.df_master.drop_duplicates(subset=[MASTER_COL_NAME], keep='first')
                master_dict = master_deduped.set_index(MASTER_COL_NAME).to_dict('index')

                mask = (df_curr['åŒ¹é…çŠ¶æ€'] == 'å¾…å¤„ç†') & (df_curr[col_map['target_name']].isin(master_dict))
                if mask.any():
                    def _fill(n): return master_dict.get(n, {})
                    matches = df_curr.loc[mask, col_map['target_name']].apply(_fill)
                    df_curr.loc[mask, 'æ ‡å‡†ç¼–ç '] = matches.apply(lambda x: x.get(MASTER_COL_CODE))
                    df_curr.loc[mask, 'æ ‡å‡†åç§°'] = df_curr.loc[mask, col_map['target_name']]
                    df_curr.loc[mask, 'æ ‡å‡†çœä»½'] = matches.apply(lambda x: x.get(MASTER_COL_PROV))
                    df_curr.loc[mask, 'æ ‡å‡†åŸå¸‚'] = matches.apply(lambda x: x.get(MASTER_COL_CITY))
                    df_curr.loc[mask, 'åŒ¹é…çŠ¶æ€'] = 'å…¨å­—åŒ¹é…'
                    df_curr.loc[mask, 'ç½®ä¿¡åº¦'] = 1.0
                    st.session_state.df_result = df_curr
                    st.rerun()
                else:
                    st.warning("æ²¡æœ‰å‘ç°å…¨å­—åŒ¹é…çš„é¡¹ç›®ï¼Œè¯·ç›´æ¥ä½¿ç”¨ AI åŒ¹é…ã€‚")
        
        if not st.session_state.processing:
            if st.button("ğŸ§  Step 2: AI èšåˆåŒ¹é…", type="primary", use_container_width=True):
                st.session_state.processing = True
                st.session_state.stop_signal = False
                st.rerun()
        else:
            if st.button("ğŸ›‘ æš‚åœä»»åŠ¡", type="secondary", use_container_width=True):
                st.session_state.stop_signal = True
                st.session_state.processing = False
                st.rerun()
                
    with col_view:
        p_bar = st.progress(0)
        status_txt = st.empty()
        table_ph = st.empty()
        table_ph.dataframe(df_curr.head(100), height=300, use_container_width=True)
        
        if st.session_state.processing:
            pending_df = df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'å¾…å¤„ç†'].copy()
            
            if pending_df.empty:
                st.session_state.processing = False
                st.success("æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæ¯•ï¼")
                st.rerun()
            
            status_txt.text("æ­£åœ¨æŒ‰åœ°åŒºèšåˆåˆ†ç»„...")
            batches = []
            
            grouped = pending_df.groupby([col_map['target_province'], col_map['target_city']])
            
            for (prov, city), group_df in grouped:
                total_in_group = len(group_df)
                for i in range(0, total_in_group, BATCH_SIZE):
                    batch_slice = group_df.iloc[i : i + BATCH_SIZE]
                    batches.append(((prov, city), batch_slice))
            
            total_batches = len(batches)
            
            # --- ä¼˜åŒ–å¹¶å‘æ•° ---
            # ç­–ç•¥: å¤šå°‘ä¸ªKeyå°±å¤šå°‘å¹¶å‘ï¼Œè‡³å°‘1ä¸ªã€‚é¿å…å¤šçº¿ç¨‹ç«äº‰åŒä¸€ä¸ªKeyå¯¼è‡´çš„429
            MAX_WORKERS = max(1, key_manager.num_keys)
            status_txt.markdown(f"**AIå¤„ç†ä¸­...** | å¯ç”¨Key: {key_manager.num_keys} | å¹¶å‘çº¿ç¨‹: {MAX_WORKERS}")
            
            completed_batches = 0
            results_buffer = []
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_map = {}
                for i, b in enumerate(batches):
                    # é¿å…ç¬é—´æäº¤æ‰€æœ‰ä»»åŠ¡ï¼Œç»™APIä¸€ç‚¹å–˜æ¯
                    if i < MAX_WORKERS: time.sleep(0.5)
                    future = executor.submit(process_batch_job, b, st.session_state.df_master, col_map, key_manager)
                    future_map[future] = i
                
                start_ts = time.time()
                
                for future in concurrent.futures.as_completed(future_map):
                    if st.session_state.stop_signal:
                        executor.shutdown(wait=False, cancel_futures=True)
                        break
                    
                    try:
                        batch_res = future.result()
                        results_buffer.extend(batch_res)
                    except Exception as e:
                        print(e)
                    
                    completed_batches += 1
                    
                    p_val = completed_batches / total_batches
                    p_bar.progress(p_val)
                    
                    elapsed = time.time() - start_ts
                    speed = (completed_batches * BATCH_SIZE) / elapsed if elapsed > 0 else 0
                    status_txt.markdown(f"**AIå¤„ç†ä¸­...** | è¿›åº¦: {completed_batches}/{total_batches} | é€Ÿåº¦: {speed:.1f} æ¡/ç§’ | è‡ªåŠ¨é™æµä¿æŠ¤å¼€å¯")
                    
                    if len(results_buffer) >= BATCH_SIZE * 2:
                        for res in results_buffer:
                            idx = res['idx']
                            for k, v in res.items():
                                if k != 'idx': df_curr.at[idx, k] = v
                        results_buffer = []
                        table_ph.dataframe(df_curr.head(50), height=300, use_container_width=True)
            
            if results_buffer:
                for res in results_buffer:
                    idx = res['idx']
                    for k, v in res.items():
                        if k != 'idx': df_curr.at[idx, k] = v
            
            st.session_state.df_result = df_curr
            st.session_state.processing = False
            st.rerun()
