import streamlit as st
import pandas as pd
import json
import warnings
import os
import time
import re
import jieba
import random
import concurrent.futures
import threading
from google import genai
from google.genai import types

# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings('ignore')

# ================= 1. åŸºç¡€é…ç½® =================

st.set_page_config(
    page_title="ChatMDM - åœ°åŒºèšåˆBatchç‰ˆ", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

# --- æ¨¡å‹é…ç½® ---
MODEL_NAME = "gemini-3-pro-preview" 

# --- å…¨å±€å¸¸é‡ ---
MASTER_COL_NAME = "åŒ»é™¢åç§°"
MASTER_COL_CODE = "åŒ»é™¢ç¼–ç "
MASTER_COL_PROV = "çœä»½"
MASTER_COL_CITY = "åŸå¸‚"
CACHE_FILE = "mdm_cache.pkl"

BATCH_SIZE = 20       # æ¯æ‰¹å¤„ç†å¤šå°‘æ¡å¾…æ¸…æ´—æ•°æ®
CANDIDATE_LIMIT = 500 # å€™é€‰æ± æœ€å¤§å®¹é‡
MAX_RETRIES = 3       

# --- API Key è§£æ ---
try:
    keys_str = st.secrets.get("GENAI_API_KEY", os.getenv("GENAI_API_KEY", ""))
    API_KEYS = [k.strip() for k in keys_str.split(',') if k.strip()]
    if not API_KEYS:
        API_KEYS = [""]
except:
    API_KEYS = [""]

# ================= 2. è§†è§‰ä½“ç³» =================

def inject_custom_css():
    st.markdown("""
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp { background-color: #050505; background-image: radial-gradient(circle at 50% 0%, #1a1a2e 0%, #050505 40%); font-family: 'Inter', sans-serif; }
        .glass-card { background: rgba(255, 255, 255, 0.03); backdrop-filter: blur(10px); border: 1px solid rgba(255, 255, 255, 0.08); border-radius: 12px; padding: 20px; margin-bottom: 20px; }
        .metric-label { font-size: 12px; color: #94a3b8; text-transform: uppercase; letter-spacing: 1px; }
        .metric-value { font-size: 28px; font-weight: 700; color: #ffffff; }
        .metric-sub { font-size: 12px; color: #64748b; margin-top: 4px; }
        [data-testid="stSidebar"] { background-color: #000000 !important; border-right: 1px solid #222; }
        [data-testid="stDataFrame"] { border: 1px solid #333; border-radius: 8px; }
        .stProgress > div > div > div > div { background-color: #3b82f6; }
        </style>
    """, unsafe_allow_html=True)

def render_metric_card(label, value, sub_text=""):
    st.markdown(f"""<div class="glass-card"><div class="metric-label">{label}</div><div class="metric-value">{value}</div><div class="metric-sub">{sub_text}</div></div>""", unsafe_allow_html=True)

# ================= 3. NLP & æ•°æ®å·¥å…· =================

STOP_WORDS = {
    "åŒ»é™¢", "æœ‰é™å…¬å¸", "æœ‰é™", "è´£ä»»", "å…¬å¸", "åˆ†é™¢", "é™„å±", 
    "å­¦", "æ ¡", "å«ç”Ÿ", "æœåŠ¡", "ä¸­å¿ƒ", "ç«™", "æ‰€", "é—¨è¯Š", "éƒ¨",
    "çœ", "å¸‚", "åŒº", "å¿", "è¡—é“", "ç¤¾åŒº"
}

def extract_core_tokens(text):
    if not isinstance(text, str): return set()
    text = re.sub(r'[ï¼ˆ(].*?[)ï¼‰]', '', text)
    words = jieba.lcut_for_search(text)
    tokens = set()
    for w in words:
        w = w.strip()
        if w not in STOP_WORDS and len(w) > 1:
            tokens.add(w)
    return tokens

class KeyManager:
    def __init__(self, api_keys):
        self.clients = []
        for k in api_keys:
            if k:
                try:
                    self.clients.append(genai.Client(api_key=k, http_options={'api_version': 'v1beta'}))
                except:
                    pass
        self.num_keys = len(self.clients)
        self.current_idx = 0
        self._lock = threading.Lock()

    def get_next_client(self):
        if self.num_keys == 0:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„ API Key")
        
        with self._lock:
            client = self.clients[self.current_idx]
            self.current_idx = (self.current_idx + 1) % self.num_keys
        return client

@st.cache_resource
def get_key_manager():
    return KeyManager(API_KEYS)

def load_cached_master():
    if os.path.exists(CACHE_FILE):
        try: return pd.read_pickle(CACHE_FILE)
        except: return None
    return None

def save_master_cache(df):
    df.to_pickle(CACHE_FILE)

def clear_master_cache():
    if os.path.exists(CACHE_FILE): os.remove(CACHE_FILE)

def process_master_data(uploaded_file):
    try:
        if uploaded_file.name.endswith('.xlsx'): df = pd.read_excel(uploaded_file, engine='openpyxl')
        else: df = pd.read_csv(uploaded_file)
        df = df.astype(str)
        df.columns = df.columns.str.strip()
        for col in df.columns: df[col] = df[col].apply(lambda x: x.strip().replace('nan', '') if x != 'nan' else '')
        
        col_map_rename = {}
        for col in df.columns:
            if "åç§°" in col and "åŒ»é™¢" in col: col_map_rename[col] = MASTER_COL_NAME
            elif "ç¼–ç " in col: col_map_rename[col] = MASTER_COL_CODE
            elif "çœ" in col: col_map_rename[col] = MASTER_COL_PROV
            elif "å¸‚" in col: col_map_rename[col] = MASTER_COL_CITY
        if col_map_rename: df = df.rename(columns=col_map_rename)
        
        with st.spinner("æ­£åœ¨æ„å»ºæœç´¢å¼•æ“ç´¢å¼•..."):
            df['tokens'] = df[MASTER_COL_NAME].apply(extract_core_tokens)
        return df, "SUCCESS"
    except Exception as e: return None, str(e)

def clean_json_response(text):
    if not text: return None
    text = text.strip()
    try: return json.loads(text)
    except: pass
    try:
        pattern = r"```(?:json)?\s*(.*?)\s*```"
        match = re.search(pattern, text, re.DOTALL)
        if match: return json.loads(match.group(1))
    except: pass
    try:
        start = text.find('[')
        end = text.rfind(']')
        if start != -1 and end != -1: return json.loads(text[start:end+1])
    except: pass
    return None

# ================= 4. AI åŠŸèƒ½å‡½æ•° (Geo & Match) =================

def call_ai_geo_standardize(key_manager, batch_df, col_map):
    """
    [æ–°å¢] ä¸“é—¨ç”¨äºæ¸…æ´—çœå¸‚çš„ AI å‡½æ•°
    """
    lines = []
    for idx, row in batch_df.iterrows():
        name = str(row[col_map['target_name']])
        prov = str(row[col_map['target_province']])
        city = str(row[col_map['target_city']])
        lines.append(f"ID:{idx} | åç§°:{name} | åŸçœ:{prov} | åŸå¸‚:{city}")
    
    data_text = "\n".join(lines)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸­å›½è¡Œæ”¿åŒºåˆ’ä¸“å®¶ã€‚è¯·æ ¹æ®ã€æœºæ„åç§°ã€‘å’Œã€åŸå§‹çœå¸‚ã€‘æ¨æ–­æ ‡å‡†çš„ã€çœä»½ã€‘å’Œã€åŸå¸‚ã€‘ã€‚
    
    ã€å¾…å¤„ç†æ•°æ®ã€‘:
    {data_text}
    
    ã€è¦æ±‚ã€‘:
    1. ä¼˜å…ˆä»"åç§°"ä¸­æå–åœ°åä¿¡æ¯ï¼ˆä¾‹å¦‚"å—äº¬å¸‚ç¬¬ä¸€åŒ»é™¢" -> æ±Ÿè‹çœ, å—äº¬å¸‚ï¼‰ã€‚
    2. å¦‚æœ"åç§°"æ— åœ°åï¼Œåˆ™å‚è€ƒ"åŸçœ/åŸå¸‚"å¹¶ä¿®æ­£é”™åˆ«å­—æˆ–è¡¥å…¨å…¨ç§°ï¼ˆå¦‚"è±«"->æ²³å—çœï¼‰ã€‚
    3. çœä»½æ ¼å¼ï¼šå¿…é¡»æ˜¯å…¨ç§°ï¼ˆå¦‚"åŒ—äº¬å¸‚"ã€"æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº"ã€"å¹¿ä¸œçœ"ï¼‰ã€‚
    4. åŸå¸‚æ ¼å¼ï¼šå¿…é¡»æ˜¯åœ°çº§å¸‚å…¨ç§°ï¼ˆå¦‚"å—äº¬å¸‚"ã€"æœé˜³åŒº"->å½’ä¸º"åŒ—äº¬å¸‚"ï¼‰ã€‚
    5. è¿”å› JSON åˆ—è¡¨ã€‚
    
    ã€è¾“å‡ºç¤ºä¾‹ã€‘:
    [
        {{"id": "0", "std_prov": "æ±Ÿè‹çœ", "std_city": "å—äº¬å¸‚"}},
        {{"id": "1", "std_prov": "åŒ—äº¬å¸‚", "std_city": "åŒ—äº¬å¸‚"}}
    ]
    """
    
    for attempt in range(3):
        try:
            client = key_manager.get_next_client()
            time.sleep(random.uniform(0.1, 0.3))
            
            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            return clean_json_response(response.text)
        except Exception as e:
            if "429" in str(e) or "503" in str(e):
                time.sleep(2 ** attempt + 1)
                continue
            else:
                break
    return []

def call_ai_batch_process(key_manager, target_batch_df, candidates_df, col_map, batch_id):
    """
    ä¸»åŒ¹é… API è°ƒç”¨ (ä¿ç•™æ‚¨çš„åŸå§‹Prompt)
    """
    cand_str_list = []
    cand_map = {}
    for _, row in candidates_df.iterrows():
        rid = str(row[MASTER_COL_CODE]).strip()
        name = row[MASTER_COL_NAME]
        cand_str_list.append(f"ID:{rid} | {name}")
        cand_map[rid] = row.to_dict()
    
    candidates_text = "\n".join(cand_str_list)

    targets_list = []
    for idx, row in target_batch_df.iterrows():
        t_name = str(row[col_map['target_name']])
        targets_list.append(f"TaskID:{str(idx)} | å¾…æ´—åç§°: {t_name}")
    
    targets_text = "\n".join(targets_list)

    # --- æ‚¨çš„åŸå§‹æç¤ºè¯ ---
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®æ¸…æ´—åŠ©æ‰‹ã€‚è¯·å°†ã€å¾…æ¸…æ´—åˆ—è¡¨ã€‘ä¸­çš„æœºæ„åç§°ï¼ŒåŒ¹é…åˆ°ã€æ ‡å‡†å€™é€‰æ± ã€‘ä¸­å”¯ä¸€çš„æœºæ„ã€‚
    
    ã€æ ‡å‡†å€™é€‰æ± ã€‘(ä»…é™ä»æ­¤åˆ—è¡¨ä¸­é€‰æ‹©):
    {candidates_text}
    
    ã€å¾…æ¸…æ´—åˆ—è¡¨ã€‘:
    {targets_text}
    
    ã€è¦æ±‚ã€‘:
    1. è¿”å›ä¸€ä¸ªJSONåˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰ TaskID çš„ç»“æœã€‚
    2. å¦‚æœåç§°é«˜åº¦ç›¸ä¼¼ï¼Œåˆ™è§†ä¸ºåŒ¹é…ã€‚
    3. å¦‚æœåœ¨å€™é€‰æ± ä¸­æ‰¾ä¸åˆ°åŒ¹é…é¡¹ï¼Œmatched_id ä¸º nullã€‚
    4. å³ä½¿å®Œå…¨ä¸åŒ¹é…ï¼Œä¹Ÿè¦è¿”å›è¯¥ TaskIDã€‚
    
    ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ã€‘:
    [
        {{"task_id": "12", "matched_id": "CODE001", "confidence": 0.95, "reason": "å…¨åä¸€è‡´"}},
        {{"task_id": "13", "matched_id": null, "confidence": 0.0, "reason": "æ— ç›¸ä¼¼é¡¹"}}
    ]
    """

    last_error = ""
    last_raw_resp = ""
    RETRIES_FOR_429 = 6 
    
    for attempt in range(RETRIES_FOR_429):
        try:
            client = key_manager.get_next_client()
            time.sleep(random.uniform(0.1, 0.3))

            response = client.models.generate_content(
                model=MODEL_NAME, 
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            
            last_raw_resp = response.text
            result_list = clean_json_response(response.text)
            
            if isinstance(result_list, list) and len(result_list) > 0:
                parsed_results = []
                for res in result_list:
                    raw_task_id = res.get('task_id')
                    if raw_task_id is None: continue
                    task_id_str = str(raw_task_id)

                    matched_id = res.get('matched_id')
                    out_row = {
                        "idx_key": task_id_str,
                        "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                        "æ ‡å‡†ç¼–ç ": None, "æ ‡å‡†åç§°": None, 
                        "æ ‡å‡†çœä»½": None, "æ ‡å‡†åŸå¸‚": None,
                        "ç½®ä¿¡åº¦": res.get('confidence', 0.0),
                        "åŒ¹é…åŸå› ": res.get('reason', 'AIæœªæ‰¾åˆ°')
                    }
                    
                    if matched_id:
                        matched_id_str = str(matched_id).strip()
                        if matched_id_str in cand_map:
                            m_row = cand_map[matched_id_str]
                            out_row.update({
                                "åŒ¹é…çŠ¶æ€": "AIåŒ¹é…",
                                "æ ‡å‡†ç¼–ç ": m_row[MASTER_COL_CODE],
                                "æ ‡å‡†åç§°": m_row[MASTER_COL_NAME],
                                "æ ‡å‡†çœä»½": m_row[MASTER_COL_PROV],
                                "æ ‡å‡†åŸå¸‚": m_row[MASTER_COL_CITY]
                            })
                    parsed_results.append(out_row)
                return parsed_results
            else:
                raise ValueError("Empty or Invalid JSON response")

        except Exception as e:
            last_error = str(e)
            if "429" in last_error or "503" in last_error or "Resource exhausted" in last_error:
                sleep_time = (2 ** attempt) + random.uniform(1, 3)
                print(f"âš ï¸ è§¦å‘é™æµ ({last_error[:20]}...), çº¿ç¨‹ä¼‘çœ  {sleep_time:.1f}s åé‡è¯•...")
                time.sleep(sleep_time)
                continue 
            else:
                if attempt < 2: 
                    time.sleep(2)
                    continue
                break
                
    return [{"error": f"{last_error} | RAW: {last_raw_resp[:50]}"}]

# ================= 5. Batch å¤„ç†é€»è¾‘ (Worker) =================

def process_geo_batch_job(batch_df, col_map, key_manager):
    """
    [æ–°å¢] åœ°åŒºæ¸…æ´—çš„ Worker
    """
    ai_res = call_ai_geo_standardize(key_manager, batch_df, col_map)
    results = []
    
    # å»ºç«‹æ˜ å°„ä»¥é˜²ä¹±åº
    res_map = {str(item['id']): item for item in ai_res if 'id' in item}
    
    for idx, _ in batch_df.iterrows():
        idx_str = str(idx)
        if idx_str in res_map:
            results.append({
                "idx": idx,
                "æ¸…æ´—åçœä»½": res_map[idx_str].get('std_prov', ''),
                "æ¸…æ´—ååŸå¸‚": res_map[idx_str].get('std_city', '')
            })
        else:
            results.append({
                "idx": idx,
                "æ¸…æ´—åçœä»½": '', 
                "æ¸…æ´—ååŸå¸‚": ''
            })
    return results

def get_batch_candidates(df_master, target_batch_df, col_map, limit=500):
    first_row = target_batch_df.iloc[0]
    
    # [ä¿®æ”¹] ä¼˜å…ˆè¯»å– AI æ¸…æ´—åçš„åˆ—ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å›é€€åˆ°åŸå§‹åˆ—
    if 'æ¸…æ´—åçœä»½' in target_batch_df.columns and pd.notna(first_row.get('æ¸…æ´—åçœä»½')) and str(first_row.get('æ¸…æ´—åçœä»½')) != '':
        t_prov = str(first_row['æ¸…æ´—åçœä»½'])
        t_city = str(first_row['æ¸…æ´—ååŸå¸‚'])
    else:
        t_prov = str(first_row.get(col_map['target_province'], ''))
        t_city = str(first_row.get(col_map['target_city'], ''))
    
    candidates = pd.DataFrame()
    if t_city and len(t_city) > 1 and t_city != 'nan' and t_city != 'æ— ':
        candidates = df_master[df_master[MASTER_COL_CITY] == t_city].copy()
    
    if candidates.empty and t_prov and len(t_prov) > 1 and t_prov != 'nan':
        candidates = df_master[df_master[MASTER_COL_PROV] == t_prov].copy()
        
    if candidates.empty:
        return pd.DataFrame(), "æ— åœ°åŒºåŒ¹é…"

    if len(candidates) > limit:
        batch_tokens = set()
        for val in target_batch_df[col_map['target_name']]:
            batch_tokens.update(extract_core_tokens(str(val)))
        
        def calc_batch_overlap(master_tokens):
            if not master_tokens: return 0
            return len(batch_tokens & master_tokens)
            
        candidates['overlap'] = candidates['tokens'].apply(calc_batch_overlap)
        candidates = candidates.sort_values('overlap', ascending=False).head(limit)
        
    return candidates, f"åŒºåŸŸ:{t_prov}-{t_city}"

def process_batch_job(batch_data, df_master, col_map, key_manager):
    """
    ä¸»åŒ¹é…çš„ Worker
    """
    (prov, city), df_batch = batch_data
    
    candidates, source_info = get_batch_candidates(df_master, df_batch, col_map, limit=CANDIDATE_LIMIT)
    
    results = []
    
    if candidates.empty:
        for idx, _ in df_batch.iterrows():
            results.append({
                "idx": idx,
                "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                "åŒ¹é…åŸå› ": f"æ ‡å‡†åº“ä¸­æ— [{prov}-{city}]æ•°æ®",
                "ç½®ä¿¡åº¦": 0.0
            })
        return results

    ai_results = call_ai_batch_process(key_manager, df_batch, candidates, col_map, f"{prov}_{city}")
    
    if len(ai_results) == 1 and "error" in ai_results[0]:
        err_msg = ai_results[0]["error"]
        for idx, _ in df_batch.iterrows():
            results.append({
                "idx": idx,
                "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                "åŒ¹é…åŸå› ": f"APIè°ƒç”¨å¤±è´¥: {err_msg}",
                "ç½®ä¿¡åº¦": 0.0
            })
        return results

    ai_res_map = {r['idx_key']: r for r in ai_results if 'idx_key' in r}
    
    final_results = []
    for idx, _ in df_batch.iterrows():
        idx_str = str(idx) 
        
        if idx_str in ai_res_map:
            res_data = ai_res_map[idx_str].copy()
            del res_data['idx_key']
            res_data['idx'] = idx
            final_results.append(res_data)
        else:
            final_results.append({
                "idx": idx,
                "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                "åŒ¹é…åŸå› ": "AIå“åº”é—æ¼(IDæœªè¿”å›)",
                "ç½®ä¿¡åº¦": 0.0
            })
            
    return final_results

# ================= 6. UI ä¸ ä¸»é€»è¾‘ =================

inject_custom_css()
key_manager = get_key_manager() 

if "df_result" not in st.session_state: st.session_state.df_result = None
if "mapping_confirmed" not in st.session_state: st.session_state.mapping_confirmed = False
if "processing" not in st.session_state: st.session_state.processing = False
if "stop_signal" not in st.session_state: st.session_state.stop_signal = False
if "col_map" not in st.session_state: st.session_state.col_map = {}
if "df_master" not in st.session_state: st.session_state.df_master = load_cached_master()
if "current_job" not in st.session_state: st.session_state.current_job = "main_match" # main_match æˆ– geo_clean

with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/3063/3063823.png", width=60)
    st.title("ChatMDM")
    st.caption("Region-Batch Edition")
    st.markdown("---")
    
    if st.session_state.df_master is not None:
        st.success(f"âœ… æ ‡å‡†åº“: {len(st.session_state.df_master):,} æ¡")
        if st.button("ğŸ—‘ï¸ é‡æ–°ä¸Šä¼ æ ‡å‡†åº“"):
            clear_master_cache()
            st.session_state.df_master = None
            st.rerun()
    else:
        master_file = st.file_uploader("ä¸Šä¼ æ ‡å‡†åº“ (xlsx/csv)", type=["xlsx", "csv"])
        if master_file:
            df_proc, msg = process_master_data(master_file)
            if df_proc is not None:
                st.session_state.df_master = df_proc
                save_master_cache(df_proc)
                st.rerun()
            else: st.error(msg)
    
    st.divider()
    if st.button("ğŸ”„ é‡ç½®æ‰€æœ‰ä»»åŠ¡"):
        bak = st.session_state.df_master
        st.session_state.clear()
        st.session_state.df_master = bak
        st.rerun()
        
    if st.session_state.df_result is not None:
        st.divider()
        df_exp = st.session_state.df_result
        csv = df_exp.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ", csv, "mdm_result.csv", "text/csv", type="primary")

st.title("ğŸ¥ åŒ»ç–—ä¸»æ•°æ®æ¸…æ´— (åœ°åŒºèšåˆ + Batchå¹¶å‘)")

if key_manager.num_keys == 0: st.error("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·åœ¨ Secrets ä¸­é…ç½® GENAI_API_KEY")
if st.session_state.df_master is None: st.info("è¯·å…ˆä¸Šä¼ æ ‡å‡†åº“"); st.stop()

# 1. ä¸Šä¼ å¾…æ´—æ•°æ®
if st.session_state.df_result is None:
    target_file = st.file_uploader("ä¸Šä¼ å¾…æ¸…æ´—æ•°æ®", type=["xlsx", "csv"])
    if target_file:
        if target_file.name.endswith('.csv'): df_t = pd.read_csv(target_file)
        else: df_t = pd.read_excel(target_file)
        df_t = df_t.astype(str)
        # åˆå§‹åŒ–åŸºç¡€åˆ—
        for c in ['åŒ¹é…çŠ¶æ€', 'æ ‡å‡†ç¼–ç ', 'æ ‡å‡†åç§°', 'æ ‡å‡†çœä»½', 'æ ‡å‡†åŸå¸‚', 'åŒ¹é…åŸå› ', 'æ¸…æ´—åçœä»½', 'æ¸…æ´—ååŸå¸‚']: 
            if c not in df_t.columns: df_t[c] = None
        df_t['åŒ¹é…çŠ¶æ€'] = 'å¾…å¤„ç†'
        df_t['ç½®ä¿¡åº¦'] = 0.0
        st.session_state.df_result = df_t
        st.rerun()

# 2. æ˜ å°„å­—æ®µ
elif not st.session_state.mapping_confirmed:
    cols = st.session_state.df_result.columns.tolist()
    c1, c2, c3 = st.columns(3)
    t_name = c1.selectbox("åç§°åˆ—", cols)
    t_prov = c2.selectbox("çœä»½åˆ—", cols)
    t_city = c3.selectbox("åŸå¸‚åˆ—", cols)
    
    if st.button("ğŸš€ å¼€å§‹æ¸…æ´—é…ç½®"):
        st.session_state.col_map = {"target_name": t_name, "target_province": t_prov, "target_city": t_city}
        st.session_state.mapping_confirmed = True
        st.rerun()

# 3. æ‰§è¡Œæ§åˆ¶å°
else:
    df_curr = st.session_state.df_result
    col_map = st.session_state.col_map
    
    done = len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] != 'å¾…å¤„ç†'])
    c1, c2, c3, c4 = st.columns(4)
    render_metric_card("æ€»è¿›åº¦", f"{done}/{len(df_curr)}")
    render_metric_card("å…¨å­—åŒ¹é…", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'å…¨å­—åŒ¹é…']))
    render_metric_card("AI å‘½ä¸­", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'AIåŒ¹é…']))
    render_metric_card("æœªå‘½ä¸­", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'AIæœªåŒ¹é…']))
    
    st.divider()
    
    col_act, col_view = st.columns([1, 4])
    
    with col_act:
        if st.button("âš¡ Step 1: ç²¾ç¡®åŒ¹é…", use_container_width=True, disabled=st.session_state.processing):
            with st.spinner("Hash ç¢°æ’ä¸­..."):
                master_deduped = st.session_state.df_master.drop_duplicates(subset=[MASTER_COL_NAME], keep='first')
                master_dict = master_deduped.set_index(MASTER_COL_NAME).to_dict('index')

                mask = (df_curr['åŒ¹é…çŠ¶æ€'] == 'å¾…å¤„ç†') & (df_curr[col_map['target_name']].isin(master_dict))
                if mask.any():
                    def _fill(n): return master_dict.get(n, {})
                    matches = df_curr.loc[mask, col_map['target_name']].apply(_fill)
                    df_curr.loc[mask, 'æ ‡å‡†ç¼–ç '] = matches.apply(lambda x: x.get(MASTER_COL_CODE))
                    df_curr.loc[mask, 'æ ‡å‡†åç§°'] = df_curr.loc[mask, col_map['target_name']]
                    df_curr.loc[mask, 'æ ‡å‡†çœä»½'] = matches.apply(lambda x: x.get(MASTER_COL_PROV))
                    df_curr.loc[mask, 'æ ‡å‡†åŸå¸‚'] = matches.apply(lambda x: x.get(MASTER_COL_CITY))
                    df_curr.loc[mask, 'åŒ¹é…çŠ¶æ€'] = 'å…¨å­—åŒ¹é…'
                    df_curr.loc[mask, 'ç½®ä¿¡åº¦'] = 1.0
                    st.session_state.df_result = df_curr
                    st.rerun()
                else:
                    st.warning("æ²¡æœ‰å‘ç°å…¨å­—åŒ¹é…çš„é¡¹ç›®ï¼Œè¯·ç›´æ¥ä½¿ç”¨ AI åŒ¹é…ã€‚")
        
        # [æ–°å¢] Step 1.5
        if not st.session_state.processing:
            if st.button("ğŸŒ Step 1.5: AI è¡¥å…¨åœ°åŒº", help="æ ¹æ®åç§°è¡¥å…¨ç¼ºå¤±çš„çœå¸‚ï¼Œå¤§å¹…æé«˜åŒ¹é…ç‡", use_container_width=True):
                st.session_state.processing = True
                st.session_state.current_job = "geo_clean"
                st.rerun()

        if not st.session_state.processing:
            if st.button("ğŸ§  Step 2: AI èšåˆåŒ¹é…", type="primary", use_container_width=True):
                st.session_state.processing = True
                st.session_state.current_job = "main_match"
                st.rerun()
        else:
            if st.button("ğŸ›‘ æš‚åœä»»åŠ¡", type="secondary", use_container_width=True):
                st.session_state.stop_signal = True
                st.session_state.processing = False
                st.rerun()
                
    with col_view:
        p_bar = st.progress(0)
        status_txt = st.empty()
        table_ph = st.empty()
        
        # å±•ç¤ºåˆ—é€‰æ‹©ï¼šå¦‚æœè¿›è¡Œäº†åœ°åŒºæ¸…æ´—ï¼Œå±•ç¤ºæ–°åˆ—
        disp_cols = [col_map['target_name'], 'åŒ¹é…çŠ¶æ€', 'æ ‡å‡†åç§°', 'ç½®ä¿¡åº¦']
        if 'æ¸…æ´—åçœä»½' in df_curr.columns:
            disp_cols = ['æ¸…æ´—åçœä»½', 'æ¸…æ´—ååŸå¸‚'] + disp_cols
        
        table_ph.dataframe(df_curr[disp_cols].head(100), height=300, use_container_width=True)
        
        if st.session_state.processing:
            MAX_WORKERS = max(1, key_manager.num_keys)
            
            # ====== åˆ†æ”¯ A: åœ°åŒºæ¸…æ´—ä»»åŠ¡ ======
            if st.session_state.current_job == "geo_clean":
                if 'æ¸…æ´—åçœä»½' not in df_curr.columns:
                    df_curr['æ¸…æ´—åçœä»½'] = df_curr[col_map['target_province']]
                    df_curr['æ¸…æ´—ååŸå¸‚'] = df_curr[col_map['target_city']]
                
                # æ‰¾å‡ºå¾…æ¸…æ´—çš„è¡Œ (æ’é™¤å·²å…¨å­—åŒ¹é…çš„)
                mask = (df_curr['åŒ¹é…çŠ¶æ€'] != 'å…¨å­—åŒ¹é…')
                target_indices = df_curr[mask].index
                
                if len(target_indices) == 0:
                    st.session_state.processing = False
                    st.success("æ²¡æœ‰éœ€è¦æ¸…æ´—çš„æ•°æ®ã€‚")
                    st.rerun()

                geo_batches = []
                temp_df = df_curr.loc[target_indices]
                for i in range(0, len(temp_df), BATCH_SIZE):
                    geo_batches.append(temp_df.iloc[i : i + BATCH_SIZE])

                status_txt.markdown(f"**AIåœ°åŒºæ¸…æ´—ä¸­...** | æ€»æ‰¹æ¬¡: {len(geo_batches)}")
                
                completed = 0
                with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    futures = {executor.submit(process_geo_batch_job, b, col_map, key_manager): b for b in geo_batches}
                    
                    for f in concurrent.futures.as_completed(futures):
                        if st.session_state.stop_signal: 
                            executor.shutdown(wait=False, cancel_futures=True)
                            break
                        
                        try:
                            results = f.result()
                            for res in results:
                                idx = res['idx']
                                df_curr.at[idx, 'æ¸…æ´—åçœä»½'] = res['æ¸…æ´—åçœä»½']
                                df_curr.at[idx, 'æ¸…æ´—ååŸå¸‚'] = res['æ¸…æ´—ååŸå¸‚']
                        except Exception as e:
                            print(e)
                            
                        completed += 1
                        p_bar.progress(completed / len(geo_batches))
                        status_txt.text(f"åœ°åŒºæ¸…æ´—è¿›åº¦: {completed}/{len(geo_batches)}")
                        
                        # åˆ·æ–°æ˜¾ç¤º
                        table_ph.dataframe(df_curr[['æ¸…æ´—åçœä»½', 'æ¸…æ´—ååŸå¸‚', col_map['target_name']]].head(50), use_container_width=True)
                
                st.session_state.df_result = df_curr
                st.session_state.processing = False
                st.success("åœ°åŒºæ¸…æ´—å®Œæˆï¼è¯·ç‚¹å‡» Step 2 è¿›è¡ŒåŒ¹é…ã€‚")
                st.rerun()

            # ====== åˆ†æ”¯ B: ä¸»åŒ¹é…ä»»åŠ¡ (ä¿®å¤ç‰ˆ) ======
            elif st.session_state.current_job == "main_match":
                pending_df = df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'å¾…å¤„ç†'].copy()
                
                if pending_df.empty:
                    st.session_state.processing = False
                    st.success("æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæ¯•ï¼(æ²¡æœ‰çŠ¶æ€ä¸º'å¾…å¤„ç†'çš„æ•°æ®)")
                    st.rerun()
                
                # 1. ç¡®å®šåˆ†ç»„åˆ— (ä¼˜å…ˆä½¿ç”¨æ¸…æ´—åçš„åˆ—)
                if 'æ¸…æ´—åçœä»½' in pending_df.columns and pending_df['æ¸…æ´—åçœä»½'].notna().any():
                    g_prov, g_city = 'æ¸…æ´—åçœä»½', 'æ¸…æ´—ååŸå¸‚'
                    status_txt.markdown("âœ… æ­£åœ¨ä½¿ç”¨ **Step 1.5 æ¸…æ´—åçš„åœ°åŒº** è¿›è¡Œæ™ºèƒ½èšåˆ...")
                else:
                    g_prov, g_city = col_map['target_province'], col_map['target_city']
                    status_txt.markdown("âš ï¸ æœªæ£€æµ‹åˆ°æ¸…æ´—åçš„åœ°åŒºæ•°æ®ï¼Œä½¿ç”¨**åŸå§‹åˆ—**è¿›è¡Œèšåˆ...")
                    
                # 2. å…³é”®ä¿®å¤ï¼šå¡«å……ç©ºå€¼å¹¶å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œé˜²æ­¢ groupby ä¸¢å¼ƒæ•°æ®
                pending_df[g_prov] = pending_df[g_prov].fillna('æœªçŸ¥çœä»½').astype(str)
                pending_df[g_city] = pending_df[g_city].fillna('æœªçŸ¥åŸå¸‚').astype(str)
                
                # 3. ç”Ÿæˆæ‰¹æ¬¡
                batches = []
                # dropna=False æ˜¯å…³é”®ï¼Œé˜²æ­¢ç©ºåœ°åŒºæ•°æ®è¢«è¿‡æ»¤
                grouped = pending_df.groupby([g_prov, g_city], dropna=False)
                
                for (prov, city), group_df in grouped:
                    # å³ä½¿åœ°åŒºä¸ºç©ºï¼Œä¹Ÿè¦å¤„ç†
                    if len(group_df) == 0: continue
                    for i in range(0, len(group_df), BATCH_SIZE):
                        batches.append(((prov, city), group_df.iloc[i : i + BATCH_SIZE]))
                
                total_batches = len(batches)
                
                if total_batches == 0:
                    st.error("âŒ ç”Ÿæˆä»»åŠ¡æ‰¹æ¬¡å¤±è´¥ï¼å¯èƒ½æ‰€æœ‰æ•°æ®åœ¨åˆ†ç»„é˜¶æ®µè¢«è¿‡æ»¤ã€‚è¯·æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©ºã€‚")
                    st.session_state.processing = False
                    st.stop()

                status_txt.markdown(f"**AIå¤„ç†ä¸­...** | å¾…å¤„ç†: {len(pending_df)}æ¡ | å…± {total_batches} ä¸ªæ‰¹æ¬¡ | æ­£åœ¨å¯åŠ¨çº¿ç¨‹...")
                
                completed_batches = 0
                results_buffer = []
                
                # 4. æ‰§è¡Œå¹¶å‘ (å¢åŠ é”™è¯¯æ•è·æ˜¾ç¤º)
                with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    future_map = {}
                    for i, b in enumerate(batches):
                        if i < MAX_WORKERS: time.sleep(0.5) # é”™å³°å¯åŠ¨
                        future = executor.submit(process_batch_job, b, st.session_state.df_master, col_map, key_manager)
                        future_map[future] = i
                    
                    start_ts = time.time()
                    
                    for future in concurrent.futures.as_completed(future_map):
                        if st.session_state.stop_signal:
                            executor.shutdown(wait=False, cancel_futures=True)
                            st.warning("ä»»åŠ¡å·²æš‚åœ")
                            break
                        
                        try:
                            batch_res = future.result()
                            if batch_res:
                                results_buffer.extend(batch_res)
                            else:
                                print("Warning: Empty batch result")
                        except Exception as e:
                            # å…³é”®ï¼šå°†åå°æŠ¥é”™æ˜¾ç¤ºåœ¨å‰å°
                            st.error(f"çº¿ç¨‹æ‰§è¡Œé”™è¯¯: {str(e)}")
                            print(f"Thread Error: {e}")
                        
                        completed_batches += 1
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        p_val = min(1.0, completed_batches / total_batches)
                        p_bar.progress(p_val)
                        
                        elapsed = time.time() - start_ts
                        speed = (completed_batches * BATCH_SIZE) / elapsed if elapsed > 0 else 0
                        status_txt.markdown(f"**AIå¤„ç†ä¸­...** | è¿›åº¦: {completed_batches}/{total_batches} | é€Ÿåº¦: {speed:.1f} æ¡/ç§’")
                        
                        # å®æ—¶å›å†™ç¼“å­˜ (æ¯æ»¡40æ¡å›å†™ä¸€æ¬¡)
                        if len(results_buffer) >= 40:
                            for res in results_buffer:
                                idx = res['idx']
                                for k, v in res.items():
                                    if k != 'idx': df_curr.at[idx, k] = v
                            results_buffer = [] # æ¸…ç©ºç¼“å­˜
                            # å¼ºåˆ¶åˆ·æ–°è¡¨æ ¼è§†å›¾
                            table_ph.dataframe(df_curr.head(50), height=300, use_container_width=True)
                
                # å¤„ç†å‰©ä½™ç»“æœ
                if results_buffer:
                    for res in results_buffer:
                        idx = res['idx']
                        for k, v in res.items():
                            if k != 'idx': df_curr.at[idx, k] = v
                
                st.session_state.df_result = df_curr
                st.session_state.processing = False
                st.success("ğŸ‰ æ‰€æœ‰åŒ¹é…ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼")
                time.sleep(1) # ç»™ç”¨æˆ·ä¸€ç‚¹æ—¶é—´çœ‹åˆ°æˆåŠŸæç¤º
                st.rerun()
