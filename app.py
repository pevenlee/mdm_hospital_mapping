import streamlit as st
import pandas as pd
import json
import warnings
import os
import time
import re
import jieba
import pickle
from google import genai
from google.genai import types
import concurrent.futures # æ–°å¢ï¼šå¹¶å‘åº“

# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings('ignore')

# ================= 1. åŸºç¡€é…ç½® =================

st.set_page_config(
    page_title="ChatMDM - æ™ºèƒ½ä¸»æ•°æ®å¯¹é½", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

# --- æ¨¡å‹é…ç½® ---
MODEL_NAME = "gemini-3-pro-preview" 

# --- å…¨å±€å¸¸é‡ ---
MASTER_COL_NAME = "åŒ»é™¢åç§°"
MASTER_COL_CODE = "åŒ»é™¢ç¼–ç "
MASTER_COL_PROV = "çœä»½"
MASTER_COL_CITY = "åŸå¸‚"
CACHE_FILE = "mdm_cache.pkl"

# ä¿®æ”¹ç‚¹ï¼šè§£æå¤šä¸ª API Key
try:
    keys_str = st.secrets.get("GENAI_API_KEY", os.getenv("GENAI_API_KEY", ""))
    # æ”¯æŒé€—å·åˆ†éš”çš„ Keyï¼Œä¾‹å¦‚ "key1,key2,key3"
    API_KEYS = [k.strip() for k in keys_str.split(',') if k.strip()]
    if not API_KEYS:
        # å¦‚æœæ²¡æœ‰é…ç½®ï¼Œé»˜è®¤ç»™ä¸€ä¸ªç©ºå­—ç¬¦ä¸²é˜²æ­¢æŠ¥é”™ï¼Œä½†å®é™…é€»è¾‘ä¸­ä¼šæ‹¦æˆª
        API_KEYS = [""]
except:
    API_KEYS = [""]

# ================= 2. è§†è§‰ä½“ç³» =================

def inject_custom_css():
    st.markdown("""
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        
        .stApp {
            background-color: #050505;
            background-image: radial-gradient(circle at 50% 0%, #1a1a2e 0%, #050505 40%);
            font-family: 'Inter', "Microsoft YaHei", sans-serif;
        }
        
        .glass-card {
            background: rgba(255, 255, 255, 0.03);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.08);
            border-radius: 12px;
            padding: 20px;
            margin-bottom: 20px;
        }
        
        .metric-label { font-size: 12px; color: #94a3b8; text-transform: uppercase; letter-spacing: 1px; }
        .metric-value { font-size: 28px; font-weight: 700; color: #ffffff; }
        .metric-sub { font-size: 12px; color: #64748b; margin-top: 4px; }
        
        [data-testid="stSidebar"] { background-color: #000000 !important; border-right: 1px solid #222; }
        [data-testid="stDataFrame"] { border: 1px solid #333; border-radius: 8px; }
        .stProgress > div > div > div > div { background-color: #3b82f6; }
        </style>
    """, unsafe_allow_html=True)

def render_metric_card(label, value, sub_text=""):
    st.markdown(f"""
    <div class="glass-card">
        <div class="metric-label">{label}</div>
        <div class="metric-value">{value}</div>
        <div class="metric-sub">{sub_text}</div>
    </div>
    """, unsafe_allow_html=True)

# ================= 3. NLP & æ•°æ®å¤„ç†å·¥å…· =================

STOP_WORDS = {
    "åŒ»é™¢", "æœ‰é™å…¬å¸", "æœ‰é™", "è´£ä»»", "å…¬å¸", "åˆ†é™¢", "é™„å±", 
    "å­¦", "æ ¡", "å«ç”Ÿ", "æœåŠ¡", "ä¸­å¿ƒ", "ç«™", "æ‰€", "é—¨è¯Š", "éƒ¨",
    "çœ", "å¸‚", "åŒº", "å¿", "è¡—é“", "ç¤¾åŒº"
}

def extract_core_tokens(text):
    if not isinstance(text, str): return set()
    text = re.sub(r'[ï¼ˆ(].*?[)ï¼‰]', '', text)
    words = jieba.lcut_for_search(text)
    tokens = set()
    for w in words:
        w = w.strip()
        if w not in STOP_WORDS and len(w) > 1:
            tokens.add(w)
    return tokens

# ä¿®æ”¹ç‚¹ï¼šè¿”å›å®¢æˆ·ç«¯åˆ—è¡¨
@st.cache_resource
def get_clients():
    clients = []
    for key in API_KEYS:
        if key:
            clients.append(genai.Client(api_key=key, http_options={'api_version': 'v1beta'}))
    return clients

# --- ç¼“å­˜ç®¡ç†å‡½æ•° ---

def load_cached_master():
    """å°è¯•ä»æœ¬åœ°åŠ è½½å·²å¤„ç†å¥½çš„ Pickle æ–‡ä»¶"""
    if os.path.exists(CACHE_FILE):
        try:
            df = pd.read_pickle(CACHE_FILE)
            return df
        except Exception as e:
            return None
    return None

def save_master_cache(df):
    """å°†å¤„ç†å¥½çš„ DataFrame (å« Tokens) ä¿å­˜åˆ°æœ¬åœ°"""
    try:
        df.to_pickle(CACHE_FILE)
    except Exception as e:
        st.error(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

def clear_master_cache():
    """æ¸…é™¤æœ¬åœ°ç¼“å­˜"""
    if os.path.exists(CACHE_FILE):
        os.remove(CACHE_FILE)

# --- æ•°æ®åŠ è½½ ---

def process_master_data(uploaded_file):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼šè¯»å– -> æ¸…æ´— -> åˆ†è¯
    """
    try:
        if uploaded_file.name.endswith('.xlsx'): 
            df = pd.read_excel(uploaded_file, engine='openpyxl')
        else: 
            df = pd.read_csv(uploaded_file)
        
        df = df.astype(str)
        df.columns = df.columns.str.strip()
        
        for col in df.columns:
            df[col] = df[col].apply(lambda x: x.strip().replace('nan', '') if x != 'nan' else '')

        col_map_rename = {}
        for col in df.columns:
            if "åç§°" in col and "åŒ»é™¢" in col: col_map_rename[col] = MASTER_COL_NAME
            elif "ç¼–ç " in col: col_map_rename[col] = MASTER_COL_CODE
            elif "çœ" in col: col_map_rename[col] = MASTER_COL_PROV
            elif "å¸‚" in col: col_map_rename[col] = MASTER_COL_CITY
        
        if col_map_rename:
            df = df.rename(columns=col_map_rename)

        required = [MASTER_COL_NAME, MASTER_COL_CODE]
        if not all(col in df.columns for col in required):
            return None, f"ç¼ºå°‘å¿…è¦åˆ—: {required}"
        
        # æ„å»ºç´¢å¼•
        with st.spinner("æ­£åœ¨æ„å»ºæœç´¢å¼•æ“ç´¢å¼•..."):
            df['tokens'] = df[MASTER_COL_NAME].apply(extract_core_tokens)
            
        return df, "SUCCESS"

    except Exception as e:
        return None, f"è¯»å–å¤±è´¥: {str(e)}"

def clean_json_response(text):
    text = text.replace("```json", "").replace("```", "").strip()
    try:
        return json.loads(text)
    except:
        return None

# ================= 4. å¬å›ä¸åŒ¹é…é€»è¾‘ =================

def get_candidates_by_keywords(df_master, target_name, top_k=15):
    target_tokens = extract_core_tokens(str(target_name))
    if not target_tokens: return pd.DataFrame()

    def calc_score(master_tokens):
        if not master_tokens: return 0.0
        intersection = len(target_tokens & master_tokens)
        union = len(target_tokens | master_tokens)
        if union == 0: return 0.0
        return intersection / union

    scores = df_master['tokens'].apply(calc_score)
    valid_mask = scores > 0.25 
    if not valid_mask.any(): return pd.DataFrame()
        
    candidates = df_master.loc[valid_mask].copy()
    candidates['sim_score'] = scores[valid_mask]
    
    candidates = candidates.sort_values('sim_score', ascending=False).head(top_k)
    candidates['__source__'] = 'å…³é”®è¯å¬å›(å¼‚åœ°/æ¨¡ç³Š)'
    return candidates

def get_candidates_smart(df_master, col_map, target_name, target_prov, target_city):
    candidates_list = []
    
    # ç­–ç•¥ A: åŒåŸ
    if target_city and target_city != "nan":
        df_geo = df_master[df_master[MASTER_COL_CITY] == target_city].copy()
        if not df_geo.empty:
            df_geo['__source__'] = 'åŒåŸèŒƒå›´'
            candidates_list.append(df_geo.head(30))

    # ç­–ç•¥ B: å…³é”®è¯
    if len(str(target_name)) >= 2:
        df_keyword = get_candidates_by_keywords(df_master, target_name, top_k=15)
        if not df_keyword.empty:
            candidates_list.append(df_keyword)

    if not candidates_list: return pd.DataFrame()
    
    final = pd.concat(candidates_list)
    final = final.drop_duplicates(subset=[MASTER_COL_CODE])
    return final

# ä¿®æ”¹ç‚¹ï¼šclient ä½œä¸ºå‚æ•°ä¼ å…¥
def call_ai_matching(client, target_name, target_prov, target_city, candidates_df):
    candidate_list_str = ""
    candidate_map = {} 
    
    for idx, row in candidates_df.iterrows():
        key = str(idx) 
        source_tag = row.get('__source__', 'æœªçŸ¥')
        info = f"ID:{key} | åç§°:{row[MASTER_COL_NAME]} | åŒºåŸŸ:{row[MASTER_COL_PROV]}-{row[MASTER_COL_CITY]} | æ¥æº:[{source_tag}]"
        candidate_list_str += info + "\n"
        candidate_map[key] = row
        
    if not candidate_list_str: return None 

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—ä¸»æ•°æ®å¯¹é½ä¸“å®¶ã€‚
    ã€ä»»åŠ¡ç›®æ ‡ã€‘åˆ¤æ–­ã€å¾…æ¸…æ´—æ•°æ®ã€‘æ˜¯å¦å¯¹åº”åˆ—è¡¨ä¸­çš„æŸå®¶æ ‡å‡†æœºæ„ã€‚
    
    ã€å¾…æ¸…æ´—æ•°æ®ã€‘
    åç§°: {target_name}
    ä½ç½®: {target_prov} - {target_city}
    
    ã€å€™é€‰åˆ—è¡¨ã€‘(æ³¨æ„æ¥æºæ ‡ç­¾)
    {candidate_list_str}
    
    ã€æ ¸å¿ƒæ¨ç†é€»è¾‘ã€‘
    1. **è¯†åˆ«æœ‰æ•ˆä¿¡æ¯**ï¼šå¾…æ¸…æ´—æ•°æ®çš„ã€åŸå¸‚ã€‘å¯èƒ½å¡«é”™ï¼Œä½†ã€åç§°ã€‘ä¸­çš„ä¸“æœ‰åè¯ï¼ˆå¦‚"åå’Œ"ã€"åè¥¿"ã€"çœç«‹"ï¼‰é€šå¸¸æ˜¯å‡†ç¡®çš„ã€‚
    2. **ä¼˜å…ˆçº§åˆ¤å®š**ï¼š
       - **Case A (åŸå¸‚é”™è¯¯ä¿®æ­£)**ï¼šå¦‚æœ `æ¥æº:[å…³é”®è¯å¬å›]` ä¸­æœ‰åç§°**é«˜åº¦ä¸€è‡´**ï¼ˆåŒ…å«ç›¸åŒçš„æ ¸å¿ƒç‰¹æŒ‡è¯ï¼‰çš„æœºæ„ï¼Œå³ä½¿åŸå¸‚ä¸ç¬¦ï¼Œä¹Ÿåº”åˆ¤å®šä¸ºåŒ¹é…ã€‚
       - **Case B (åŒåŸå¸¸è§„åŒ¹é…)**ï¼šåœ¨ `æ¥æº:[åŒåŸèŒƒå›´]` ä¸­å¯»æ‰¾åç§°å«ä¹‰ä¸€è‡´çš„æœºæ„ã€‚
    3. **ç±»å‹ä¸€è‡´æ€§æ ¡éªŒ**ï¼šä¸¥ç¦å°†"å«ç”Ÿå®¤"åŒ¹é…åˆ°"ç»¼åˆåŒ»é™¢"ã€‚
    4. **æ— æ³•ç¡®å®š**ï¼šè¿”å› nullã€‚
    
    ã€è¾“å‡º JSON æ ¼å¼ã€‘
    {{
        "matched_id": "å€™é€‰ID (String) æˆ– null",
        "confidence": 0.0-1.0,
        "reason": "ç®€è¿°ç†ç”±"
    }}
    """
    
    try:
        response = client.models.generate_content(
            model=MODEL_NAME, 
            contents=prompt,
            config=types.GenerateContentConfig(response_mime_type="application/json")
        )
        result = clean_json_response(response.text)
        
        if result and result.get('matched_id'):
            matched_id = str(result['matched_id'])
            if matched_id in candidate_map:
                matched_row = candidate_map[matched_id]
                return {
                    "æ ‡å‡†ç¼–ç ": matched_row[MASTER_COL_CODE],
                    "æ ‡å‡†åç§°": matched_row[MASTER_COL_NAME],
                    "æ ‡å‡†çœä»½": matched_row[MASTER_COL_PROV],
                    "æ ‡å‡†åŸå¸‚": matched_row[MASTER_COL_CITY],
                    "ç½®ä¿¡åº¦": result.get('confidence', 0.5),
                    "åŒ¹é…åŸå› ": result.get('reason', 'AIæ¨ç†'),
                    "åŒ¹é…çŠ¶æ€": "AIåŒ¹é…"
                }
        return {"åŒ¹é…åŸå› ": result.get('reason', 'æœªåœ¨å€™é€‰ä¸­æ‰¾åˆ°') if result else "AIè¿”å›æ ¼å¼æ— æ•ˆ", "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…"}
            
    except Exception as e:
        return {"åŒ¹é…åŸå› ": f"APIå¼‚å¸¸: {str(e)}", "åŒ¹é…çŠ¶æ€": "é”™è¯¯"}

# ä¿®æ”¹ç‚¹ï¼šæ–°å¢çº¿ç¨‹ä»»åŠ¡å‡½æ•°
def process_row_job(idx, row_data, df_master, col_map, client):
    """
    çº¿ç¨‹æ‰§è¡Œçš„ä»»åŠ¡å‡½æ•°
    """
    # é˜²æ­¢å¤šçº¿ç¨‹ç¯å¢ƒä¸‹pandas SettingWithCopyWarningç­‰ï¼Œè¿™é‡Œæ“ä½œåŸå§‹æ•°æ®çš„å‰¯æœ¬
    t_n = str(row_data[col_map['target_name']])
    t_p = str(row_data[col_map['target_province']]) if col_map['target_province'] != "æ— " else ""
    t_c = str(row_data[col_map['target_city']]) if col_map['target_city'] != "æ— " else ""
    
    # å¬å›
    candidates = get_candidates_smart(df_master, col_map, t_n, t_p, t_c)
    
    result = {
        "åŒ¹é…çŠ¶æ€": "æ— å€™é€‰",
        "åŒ¹é…åŸå› ": "åŒåŸ/å…³é”®è¯å‡æœªå¬å›è¿‘ä¼¼æ•°æ®",
        "æ ‡å‡†ç¼–ç ": None,
        "æ ‡å‡†åç§°": None,
        "æ ‡å‡†çœä»½": None,
        "æ ‡å‡†åŸå¸‚": None,
        "ç½®ä¿¡åº¦": 0.0
    }
    
    # åŒ¹é…
    if len(candidates) > 0:
        ai_res = call_ai_matching(client, t_n, t_p, t_c, candidates)
        # æ›´æ–°ç»“æœå­—å…¸
        result.update(ai_res)
        
    return idx, result

# ================= 5. åˆå§‹åŒ–ä¸ä¾§è¾¹æ é€»è¾‘ =================

inject_custom_css()
clients = get_clients() # åˆå§‹åŒ–å®¢æˆ·ç«¯æ± 

# åˆå§‹åŒ– Session State
if "df_result" not in st.session_state: st.session_state.df_result = None
if "mapping_confirmed" not in st.session_state: st.session_state.mapping_confirmed = False
if "processing" not in st.session_state: st.session_state.processing = False
if "stop_signal" not in st.session_state: st.session_state.stop_signal = False
if "col_map" not in st.session_state: st.session_state.col_map = {}
# åˆå§‹åŒ– Master Data (å°è¯•ä»ç¼“å­˜åŠ è½½)
if "df_master" not in st.session_state: 
    st.session_state.df_master = load_cached_master()

with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/3063/3063823.png", width=60)
    st.title("ChatMDM")
    st.caption("Multi-Key Queue Edition")
    st.markdown("---")

    st.markdown("### 1ï¸âƒ£ æ ‡å‡†åº“ç®¡ç†")
    
    # é€»è¾‘åˆ†æ”¯ï¼šæœ‰ç¼“å­˜ vs æ— ç¼“å­˜
    if st.session_state.df_master is not None:
        st.success(f"âœ… å·²åŠ è½½ç¼“å­˜æ ‡å‡†åº“\n\næ•°æ®é‡: {len(st.session_state.df_master):,} æ¡")
        
        if st.button("ğŸ—‘ï¸ åˆ é™¤ç¼“å­˜ / æ›´æ¢æ–‡ä»¶"):
            clear_master_cache()
            st.session_state.df_master = None
            st.rerun()
    else:
        st.info("é¦–æ¬¡è¿è¡Œè¯·ä¸Šä¼  mdm.xlsx")
        master_file = st.file_uploader("ä¸Šä¼ æ–‡ä»¶ (è‡ªåŠ¨å»ºç«‹ç´¢å¼•ç¼“å­˜)", type=["xlsx", "csv"], key="master_uploader")

        if master_file:
            df_processed, msg = process_master_data(master_file)
            if df_processed is not None:
                st.session_state.df_master = df_processed
                # ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜
                save_master_cache(df_processed)
                st.success("ç´¢å¼•æ„å»ºå®Œæˆå¹¶å·²ç¼“å­˜ï¼")
                time.sleep(1)
                st.rerun()
            else:
                st.error(msg)

    st.markdown("---")
    
    # é‡ç½®ä»»åŠ¡æŒ‰é’®
    if st.button("ğŸ”„ é‡ç½®ä»»åŠ¡ (ä¿ç•™æ ‡å‡†åº“)", use_container_width=True):
        saved_master = st.session_state.df_master
        st.session_state.clear()
        st.session_state.df_master = saved_master
        st.rerun()
        
    if st.session_state.df_result is not None:
        st.divider()
        st.markdown("### ğŸ“¥ ç»“æœå¯¼å‡º")
        df_exp = st.session_state.df_result
        done_cnt = len(df_exp[df_exp['åŒ¹é…çŠ¶æ€'] != 'å¾…å¤„ç†'])
        match_cnt = len(df_exp[df_exp['æ ‡å‡†ç¼–ç '].notna()])
        st.caption(f"è¿›åº¦: {done_cnt}/{len(df_exp)} | å‘½ä¸­: {match_cnt}")
        csv = df_exp.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ä¸‹è½½ç»“æœ", data=csv, file_name="mdm_result.csv", mime="text/csv", type="primary")

# ================= 6. ä¸»é€»è¾‘ =================

st.title("ğŸ¥ åŒ»ç–—ä¸»æ•°æ®æ™ºèƒ½å¯¹é½ç³»ç»Ÿ (å¤šçº¿ç¨‹ç‰ˆ)")

if not clients or (len(clients) == 1 and not clients[0].api_key):
    st.warning("âš ï¸ è¯·é…ç½® GENAI_API_KEY (æ”¯æŒé€—å·åˆ†éš”å¤šä¸ªKey)")

# æ£€æŸ¥ Master Data æ˜¯å¦å°±ç»ª
if st.session_state.df_master is None:
    st.info("ğŸ‘‹ æ¬¢è¿ï¼è¯·ä»å·¦ä¾§ä¸Šä¼ æ ‡å‡†åº“ä»¥å¼€å§‹ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨ç¼“å­˜æ–‡ä»¶ï¼Œåç»­æ— éœ€é‡å¤ä¸Šä¼ ã€‚")
    st.stop()
else:
    df_master = st.session_state.df_master 

# --- Phase 1: ä¸Šä¼  ---
if st.session_state.df_result is None:
    st.markdown("### 2ï¸âƒ£ ä¸Šä¼ å¾…æ¸…æ´—æ•°æ®")
    uploaded_file = st.file_uploader("æ”¯æŒ Excel / CSV", type=["xlsx", "csv"], key="target_uploader")
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'): df_temp = pd.read_csv(uploaded_file)
            else: df_temp = pd.read_excel(uploaded_file, engine='openpyxl')
            
            df_temp = df_temp.astype(str)
            for col in ['åŒ¹é…çŠ¶æ€', 'æ ‡å‡†ç¼–ç ', 'æ ‡å‡†åç§°', 'æ ‡å‡†çœä»½', 'æ ‡å‡†åŸå¸‚', 'åŒ¹é…åŸå› ']:
                df_temp[col] = None
            df_temp['åŒ¹é…çŠ¶æ€'] = 'å¾…å¤„ç†'
            df_temp['ç½®ä¿¡åº¦'] = 0.0
            
            st.session_state.df_result = df_temp
            st.rerun()
        except Exception as e:
            st.error(f"è¯»å–å¤±è´¥: {e}")

# --- Phase 2: æ˜ å°„ ---
elif not st.session_state.mapping_confirmed:
    st.markdown("### 3ï¸âƒ£ å­—æ®µæ˜ å°„")
    df_cols = st.session_state.df_result.columns.tolist()
    c1, c2, c3 = st.columns(3)
    with c1: t_name = st.selectbox("ã€åç§°ã€‘åˆ—", df_cols)
    with c2: t_prov = st.selectbox("ã€çœä»½ã€‘åˆ— (å¯é€‰)", ["æ— "] + df_cols)
    with c3: t_city = st.selectbox("ã€åŸå¸‚ã€‘åˆ— (å¯é€‰)", ["æ— "] + df_cols)

    if st.button("å¼€å§‹å¤„ç†", type="primary"):
        st.session_state.col_map = {"target_name": t_name, "target_province": t_prov, "target_city": t_city}
        st.session_state.mapping_confirmed = True
        st.rerun()

# --- Phase 3: æ§åˆ¶å° (å¹¶è¡Œå¤„ç†) ---
else:
    df_curr = st.session_state.df_result
    col_map = st.session_state.col_map
    
    total = len(df_curr)
    done_cnt = len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] != 'å¾…å¤„ç†'])
    
    c1, c2, c3, c4 = st.columns(4)
    with c1: render_metric_card("è¿›åº¦", f"{done_cnt}/{total}")
    with c2: render_metric_card("å…¨å­—åŒ¹é…", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'å…¨å­—åŒ¹é…']))
    with c3: render_metric_card("AI åŒ¹é…", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'AIåŒ¹é…']))
    with c4: render_metric_card("æœªåŒ¹é…", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'AIæœªåŒ¹é…']))
    
    st.divider()
    
    col_ctrl, col_status = st.columns([1, 3])
    with col_ctrl:
        if st.button("âš¡ ç²¾ç¡®åŒ¹é… (Hash)", use_container_width=True, disabled=st.session_state.processing):
            with st.spinner("Hash æ¯”å¯¹ä¸­..."):
                t_name = col_map['target_name']
                master_dict = {str(k).strip(): v for k, v in df_master.drop_duplicates(subset=[MASTER_COL_NAME]).set_index(MASTER_COL_NAME).to_dict('index').items()}
                
                for idx, row in df_curr.iterrows():
                    if row['åŒ¹é…çŠ¶æ€'] != 'å¾…å¤„ç†': continue
                    val = str(row[t_name]).strip()
                    if val in master_dict:
                        match = master_dict[val]
                        df_curr.at[idx, 'æ ‡å‡†ç¼–ç '] = match.get(MASTER_COL_CODE)
                        df_curr.at[idx, 'æ ‡å‡†åç§°'] = val
                        df_curr.at[idx, 'æ ‡å‡†çœä»½'] = match.get(MASTER_COL_PROV)
                        df_curr.at[idx, 'æ ‡å‡†åŸå¸‚'] = match.get(MASTER_COL_CITY)
                        df_curr.at[idx, 'åŒ¹é…çŠ¶æ€'] = 'å…¨å­—åŒ¹é…'
                        df_curr.at[idx, 'ç½®ä¿¡åº¦'] = 1.0
                st.session_state.df_result = df_curr
                st.rerun()

        if not st.session_state.processing:
            if st.button("ğŸ§  AI æ·±åº¦åŒ¹é… (5è·¯å¹¶å‘)", type="primary", use_container_width=True):
                if not clients or (len(clients)==1 and not clients[0].api_key):
                    st.error("æœªé…ç½®æœ‰æ•ˆçš„API Key")
                else:
                    st.session_state.processing = True
                    # åœ¨ st.session_state.processing = True ä¹‹å
# 1. å…ˆåœ¨ä¸»çº¿ç¨‹ï¼ˆåˆ©ç”¨Pandaså‘é‡åŒ–ï¼‰å®Œæˆå¬å›ï¼Œä¸è¦åœ¨çº¿ç¨‹æ± é‡Œç®—
with st.spinner("æ­£åœ¨é¢„å¤„ç†å¬å›æ•°æ®..."):
    # å‡è®¾æˆ‘ä»¬æŠŠå¬å›ç»“æœå­˜å…¥ df_curr çš„ä¸€ä¸ªä¸´æ—¶åˆ—
    df_curr['candidates'] = df_curr.apply(lambda r: get_candidates_smart(...), axis=1)

# 2. çº¿ç¨‹æ± åªè´Ÿè´£çº¯ç½‘ç»œè¯·æ±‚ï¼ˆAPIè°ƒç”¨ï¼‰
def process_row_job(idx, row_data, client, candidates):
    # è¿™é‡Œåªå†™ call_ai_matchingï¼Œä¸å†™å…¶ä»–çš„è®¡ç®—
    ai_res = call_ai_matching(client, ..., candidates)
    return idx, ai_res
                    st.session_state.stop_signal = False
                    st.rerun()
        else:
            if st.button("ğŸ›‘ æš‚åœ", type="secondary", use_container_width=True):
                st.session_state.stop_signal = True
                st.session_state.processing = False
                st.rerun()

    with col_status:
        progress_bar = st.progress(0)
        status_text = st.empty()
        table_placeholder = st.empty()
        
        table_placeholder.dataframe(
            df_curr, 
            use_container_width=True, 
            column_order=['åŒ¹é…çŠ¶æ€', 'ç½®ä¿¡åº¦', 'åŒ¹é…åŸå› ', col_map['target_name'], 'æ ‡å‡†åç§°'],
            height=300
        )
        
        if st.session_state.processing:
            pending_indices = df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'å¾…å¤„ç†'].index.tolist()
            
            if not pending_indices:
                st.session_state.processing = False
                st.success("å…¨éƒ¨å®Œæˆ")
                st.rerun()
            
            # çº¿ç¨‹æ± å¹¶å‘é€»è¾‘
            max_workers = min(len(clients), 5) # æœ€å¤š5è·¯å¹¶å‘ï¼Œä¸”ä¸è¶…è¿‡Keyçš„æ•°é‡
            if max_workers == 0: max_workers = 1 # å®¹é”™

            completed_count = 0
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_idx = {}
                
                # æäº¤ä»»åŠ¡
                for i, idx in enumerate(pending_indices):
                    if st.session_state.stop_signal:
                        break
                    
                    # å¾ªç¯åˆ†é…å®¢æˆ·ç«¯
                    client = clients[i % len(clients)]
                    row_data = df_curr.loc[idx].to_dict()
                    
                    future = executor.submit(
                        process_row_job,
                        idx, row_data, df_master, col_map, client
                    )
                    future_to_idx[future] = idx
                
                # å¤„ç†è¿”å›ç»“æœ
                for future in concurrent.futures.as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    
                    try:
                        _, result = future.result()
                        
                        # æ›´æ–° DataFrame
                        for k, v in result.items():
                            df_curr.at[idx, k] = v
                            
                    except Exception as e:
                        df_curr.at[idx, 'åŒ¹é…çŠ¶æ€'] = 'çº¿ç¨‹é”™è¯¯'
                        df_curr.at[idx, 'åŒ¹é…åŸå› '] = str(e)
                    
                    completed_count += 1
                    
                    # å®æ—¶æ›´æ–°è¿›åº¦
                    status_text.text(f"æ­£åœ¨å¤„ç† (å¹¶å‘è·¯æ•°: {max_workers}): å·²å®Œæˆ {completed_count}/{len(pending_indices)}")
                    progress_bar.progress(completed_count / len(pending_indices))
                    
                    # åˆ†æ‰¹æ¬¡åˆ·æ–°UI (æ¯5ä¸ªç»“æœåˆ·æ–°ä¸€æ¬¡ï¼Œé¿å…è¿‡äºé¢‘ç¹çš„rerunå¯¼è‡´å¡é¡¿)
                    if completed_count % 20 == 0:
                        st.session_state.df_result = df_curr
                        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸è°ƒç”¨st.rerun()ï¼Œå› ä¸ºçº¿ç¨‹è¿˜åœ¨è¿è¡Œã€‚
                        # æˆ‘ä»¬é€šè¿‡æ›´æ–°st.session_stateå’Œä¸Šé¢çš„progress_bar/status_textæ¥åé¦ˆã€‚
                        # åªæœ‰å½“è¿™ä¸€æ‰¹æ¬¡ä»»åŠ¡å…¨éƒ¨æäº¤å®Œæ¯•ï¼Œæˆ–è€…æ‰‹åŠ¨è§¦å‘æ—¶ï¼Œæ‰ä¼šå®Œå…¨é‡ç»˜è¡¨æ ¼ã€‚
                        table_placeholder.dataframe(
                            df_curr, 
                            use_container_width=True, 
                            column_order=['åŒ¹é…çŠ¶æ€', 'ç½®ä¿¡åº¦', 'åŒ¹é…åŸå› ', col_map['target_name'], 'æ ‡å‡†åç§°'],
                            height=300
                        )

            # è¿™ä¸€è½®æ‰€æœ‰ä»»åŠ¡æäº¤å¹¶å¤„ç†å®Œæ¯•åï¼Œä¿å­˜æœ€ç»ˆçŠ¶æ€å¹¶é‡ç»˜
            st.session_state.df_result = df_curr
            st.session_state.processing = False
            
            if not st.session_state.stop_signal:
                st.success("é˜Ÿåˆ—å¤„ç†å®Œæ¯•")
            else:
                st.warning("å¤„ç†å·²æš‚åœ")
                
            st.rerun()

