import streamlit as st
import pandas as pd
import json
import warnings
import os
import time
import re
import jieba
import random
import concurrent.futures
import math
from google import genai
from google.genai import types

# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings('ignore')

# ================= 1. åŸºç¡€é…ç½® =================

st.set_page_config(
    page_title="ChatMDM - åœ°åŒºèšåˆBatchç‰ˆ", 
    layout="wide", 
    initial_sidebar_state="expanded"
)

# --- æ¨¡å‹é…ç½® ---
# å¿…é¡»ä½¿ç”¨ flash æ¨¡å‹ï¼Œå› ä¸º Batch æ¨¡å¼ä¸‹ä¸Šä¸‹æ–‡çª—å£ï¼ˆContext Windowï¼‰éœ€æ±‚è¾ƒå¤§
MODEL_NAME = "gemini-3-pro-preview" 

# --- å…¨å±€å¸¸é‡ ---
MASTER_COL_NAME = "åŒ»é™¢åç§°"
MASTER_COL_CODE = "åŒ»é™¢ç¼–ç "
MASTER_COL_PROV = "çœä»½"
MASTER_COL_CITY = "åŸå¸‚"
CACHE_FILE = "mdm_cache.pkl"

BATCH_SIZE = 20       # æ¯æ‰¹å¤„ç†å¤šå°‘æ¡å¾…æ¸…æ´—æ•°æ®
CANDIDATE_LIMIT = 300 # å€™é€‰æ± æœ€å¤§å®¹é‡
MAX_RETRIES = 3       # API é‡è¯•æ¬¡æ•°

# --- API Key è§£æ ---
try:
    keys_str = st.secrets.get("GENAI_API_KEY", os.getenv("GENAI_API_KEY", ""))
    API_KEYS = [k.strip() for k in keys_str.split(',') if k.strip()]
    if not API_KEYS:
        API_KEYS = [""]
except:
    API_KEYS = [""]

# ================= 2. è§†è§‰ä½“ç³» (ä¿æŒä¸å˜) =================

def inject_custom_css():
    st.markdown("""
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        .stApp { background-color: #050505; background-image: radial-gradient(circle at 50% 0%, #1a1a2e 0%, #050505 40%); font-family: 'Inter', sans-serif; }
        .glass-card { background: rgba(255, 255, 255, 0.03); backdrop-filter: blur(10px); border: 1px solid rgba(255, 255, 255, 0.08); border-radius: 12px; padding: 20px; margin-bottom: 20px; }
        .metric-label { font-size: 12px; color: #94a3b8; text-transform: uppercase; letter-spacing: 1px; }
        .metric-value { font-size: 28px; font-weight: 700; color: #ffffff; }
        .metric-sub { font-size: 12px; color: #64748b; margin-top: 4px; }
        [data-testid="stSidebar"] { background-color: #000000 !important; border-right: 1px solid #222; }
        [data-testid="stDataFrame"] { border: 1px solid #333; border-radius: 8px; }
        .stProgress > div > div > div > div { background-color: #3b82f6; }
        </style>
    """, unsafe_allow_html=True)

def render_metric_card(label, value, sub_text=""):
    st.markdown(f"""<div class="glass-card"><div class="metric-label">{label}</div><div class="metric-value">{value}</div><div class="metric-sub">{sub_text}</div></div>""", unsafe_allow_html=True)

# ================= 3. NLP & æ•°æ®å·¥å…· =================

STOP_WORDS = {
    "åŒ»é™¢", "æœ‰é™å…¬å¸", "æœ‰é™", "è´£ä»»", "å…¬å¸", "åˆ†é™¢", "é™„å±", 
    "å­¦", "æ ¡", "å«ç”Ÿ", "æœåŠ¡", "ä¸­å¿ƒ", "ç«™", "æ‰€", "é—¨è¯Š", "éƒ¨",
    "çœ", "å¸‚", "åŒº", "å¿", "è¡—é“", "ç¤¾åŒº"
}

def extract_core_tokens(text):
    if not isinstance(text, str): return set()
    text = re.sub(r'[ï¼ˆ(].*?[)ï¼‰]', '', text)
    words = jieba.lcut_for_search(text)
    tokens = set()
    for w in words:
        w = w.strip()
        if w not in STOP_WORDS and len(w) > 1:
            tokens.add(w)
    return tokens

@st.cache_resource
def get_clients():
    clients = []
    for key in API_KEYS:
        if key:
            clients.append(genai.Client(api_key=key, http_options={'api_version': 'v1beta'}))
    return clients

def load_cached_master():
    if os.path.exists(CACHE_FILE):
        try: return pd.read_pickle(CACHE_FILE)
        except: return None
    return None

def save_master_cache(df):
    df.to_pickle(CACHE_FILE)

def clear_master_cache():
    if os.path.exists(CACHE_FILE): os.remove(CACHE_FILE)

def process_master_data(uploaded_file):
    try:
        if uploaded_file.name.endswith('.xlsx'): df = pd.read_excel(uploaded_file, engine='openpyxl')
        else: df = pd.read_csv(uploaded_file)
        df = df.astype(str)
        df.columns = df.columns.str.strip()
        for col in df.columns: df[col] = df[col].apply(lambda x: x.strip().replace('nan', '') if x != 'nan' else '')
        
        col_map_rename = {}
        for col in df.columns:
            if "åç§°" in col and "åŒ»é™¢" in col: col_map_rename[col] = MASTER_COL_NAME
            elif "ç¼–ç " in col: col_map_rename[col] = MASTER_COL_CODE
            elif "çœ" in col: col_map_rename[col] = MASTER_COL_PROV
            elif "å¸‚" in col: col_map_rename[col] = MASTER_COL_CITY
        if col_map_rename: df = df.rename(columns=col_map_rename)
        
        with st.spinner("æ­£åœ¨æ„å»ºæœç´¢å¼•æ“ç´¢å¼•..."):
            df['tokens'] = df[MASTER_COL_NAME].apply(extract_core_tokens)
        return df, "SUCCESS"
    except Exception as e: return None, str(e)

def clean_json_response(text):
    text = re.sub(r'^.*?```json', '', text, flags=re.DOTALL)
    text = re.sub(r'```.*$', '', text, flags=re.DOTALL)
    text = text.strip()
    try: return json.loads(text)
    except: return None

# ================= 4. æ‰¹é‡æ™ºèƒ½åŒ¹é…é€»è¾‘ (Batch Logic) =================

def get_batch_candidates(df_master, target_batch_df, col_map, limit=500):
    """
    æ™ºèƒ½å€™é€‰æ± æ„å»ºï¼š
    1. é”å®šè¯¥ Batch æ‰€åœ¨çš„åŸå¸‚ã€‚
    2. å¦‚æœåŸå¸‚æ•°æ®è¿‡å¤šï¼ŒåŸºäº Batch ä¸­æ‰€æœ‰å¾…æ¸…æ´—æ•°æ®çš„å…³é”®è¯å¹¶é›†è¿›è¡Œå¬å›ï¼Œç¡®ä¿ Top 500 åŒ…å«æ­£ç¡®ç­”æ¡ˆã€‚
    """
    # å‡è®¾ Batch å†…çš„æ•°æ®éƒ½æ˜¯åŒä¸€ä¸ªåŸå¸‚ï¼ˆè°ƒåº¦å±‚ä¿è¯ï¼‰
    first_row = target_batch_df.iloc[0]
    t_prov = str(first_row.get(col_map['target_province'], ''))
    t_city = str(first_row.get(col_map['target_city'], ''))
    
    # 1. åŒºåŸŸè¿‡æ»¤
    candidates = pd.DataFrame()
    if t_city and len(t_city) > 1 and t_city != 'nan' and t_city != 'æ— ':
        candidates = df_master[df_master[MASTER_COL_CITY] == t_city].copy()
    
    # å¦‚æœåŸå¸‚æ²¡æ‰¾åˆ°ï¼Œæˆ–è€…åŸå¸‚æœªå¡«å†™ï¼Œå°è¯•ç”¨çœä»½
    if candidates.empty and t_prov and len(t_prov) > 1 and t_prov != 'nan':
        candidates = df_master[df_master[MASTER_COL_PROV] == t_prov].copy()
        
    # å¦‚æœè¿˜æ˜¯ç©ºçš„ï¼ˆå®Œå…¨æ²¡å¡«åœ°åŒºï¼‰ï¼Œæˆ–è€…æ•°é‡å¤ªå°‘ï¼Œå…¨åº“ï¼ˆæå…¶ç½•è§ï¼Œæš‚ä¸å¤„ç†ä»¥ä¿é€Ÿåº¦ï¼‰
    if candidates.empty:
        # å…œåº•ï¼šå¦‚æœå®Œå…¨æ²¡æœ‰åœ°åŒºä¿¡æ¯ï¼Œä½¿ç”¨å…³é”®è¯å¬å›ï¼ˆé’ˆå¯¹ Batch ä¸­æ¯æ¡åˆ†åˆ«å¬å›å†åˆå¹¶ï¼‰
        # ä½†ä¸ºä¿è¯é€Ÿåº¦ï¼Œè¿™é‡Œè¿”å›ç©ºï¼Œç”± Prompt å¤„ç†ä¸ºâ€œæœªæ‰¾åˆ°â€
        return pd.DataFrame(), "æ— åœ°åŒºåŒ¹é…"

    # 2. æ•°é‡æ§åˆ¶ (Smart Pruning)
    if len(candidates) > limit:
        # æ”¶é›† Batch ä¸­æ‰€æœ‰å¾…æŸ¥è¯¢åç§°çš„ Token å¹¶é›†
        batch_tokens = set()
        for val in target_batch_df[col_map['target_name']]:
            batch_tokens.update(extract_core_tokens(str(val)))
        
        # è®¡ç®—è¯¥åœ°åŒºå€™é€‰æœºæ„ä¸ Token å¹¶é›†çš„é‡å åº¦
        def calc_batch_overlap(master_tokens):
            if not master_tokens: return 0
            return len(batch_tokens & master_tokens)
            
        candidates['overlap'] = candidates['tokens'].apply(calc_batch_overlap)
        # å–é‡å åº¦é«˜çš„ + éšæœºè¡¥å……ï¼ˆé˜²æ­¢å…¨0ï¼‰
        candidates = candidates.sort_values('overlap', ascending=False).head(limit)
        
    return candidates, f"åŒºåŸŸ:{t_prov}-{t_city}"

def call_ai_batch_process(clients, target_batch_df, candidates_df, col_map, batch_id):
    """
    Batch API è°ƒç”¨
    """
    # 1. æ„å»ºå€™é€‰æ± å­—ç¬¦ä¸²
    cand_str_list = []
    cand_map = {}
    for _, row in candidates_df.iterrows():
        rid = str(row[MASTER_COL_CODE]) # ä½¿ç”¨æ ‡å‡†ç¼–ç ä½œä¸ºID
        name = row[MASTER_COL_NAME]
        cand_str_list.append(f"ID:{rid} | {name}")
        cand_map[rid] = row.to_dict()
    
    candidates_text = "\n".join(cand_str_list)

    # 2. æ„å»ºå¾…æ¸…æ´—åˆ—è¡¨å­—ç¬¦ä¸²
    targets_list = []
    for idx, row in target_batch_df.iterrows():
        t_name = str(row[col_map['target_name']])
        targets_list.append(f"TaskID:{idx} | å¾…æ´—åç§°: {t_name}")
    
    targets_text = "\n".join(targets_list)

    # 3. Prompt
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®æ¸…æ´—åŠ©æ‰‹ã€‚è¯·å°†ã€å¾…æ¸…æ´—åˆ—è¡¨ã€‘ä¸­çš„æœºæ„åç§°ï¼ŒåŒ¹é…åˆ°ã€æ ‡å‡†å€™é€‰æ± ã€‘ä¸­å”¯ä¸€çš„æœºæ„ã€‚
    
    ã€æ ‡å‡†å€™é€‰æ± ã€‘(ä»…é™ä»æ­¤åˆ—è¡¨ä¸­é€‰æ‹©):
    {candidates_text}
    
    ã€å¾…æ¸…æ´—åˆ—è¡¨ã€‘:
    {targets_text}
    
    ã€è¦æ±‚ã€‘:
    1. è¿”å›ä¸€ä¸ªJSONåˆ—è¡¨ï¼ŒåŒ…å«æ‰€æœ‰ TaskID çš„ç»“æœã€‚
    2. å¦‚æœåç§°é«˜åº¦ç›¸ä¼¼ï¼ˆå¿½ç•¥åˆ†é™¢ã€æœ‰é™å…¬å¸åç¼€ç­‰å·®å¼‚ï¼‰ï¼Œåˆ™è§†ä¸ºåŒ¹é…ã€‚
    3. å¦‚æœåœ¨å€™é€‰æ± ä¸­æ‰¾ä¸åˆ°åŒ¹é…é¡¹ï¼Œmatched_id ä¸º nullã€‚
    4. å³ä½¿å®Œå…¨ä¸åŒ¹é…ï¼Œä¹Ÿè¦è¿”å›è¯¥ TaskIDã€‚
    
    ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ã€‘:
    [
        {{"task_id": "12", "matched_id": "CODE001", "confidence": 0.95, "reason": "å…¨åä¸€è‡´"}},
        {{"task_id": "13", "matched_id": null, "confidence": 0.0, "reason": "æ— ç›¸ä¼¼é¡¹"}}
    ]
    """

    last_error = ""
    
    # 4. é‡è¯•å¾ªç¯
    for attempt in range(MAX_RETRIES):
        try:
            client = random.choice(clients)
            # Jitter
            time.sleep(random.uniform(0.1, 0.5) + attempt) 
            
            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            
            result_list = clean_json_response(response.text)
            
            if isinstance(result_list, list):
                # è§£æç»“æœ
                parsed_results = []
                for res in result_list:
                    task_id = res.get('task_id')
                    if task_id is None: continue
                    
                    matched_id = res.get('matched_id')
                    out_row = {
                        "idx": int(task_id), # è¿˜åŸå› DataFrame çš„ Index
                        "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                        "æ ‡å‡†ç¼–ç ": None, "æ ‡å‡†åç§°": None, 
                        "æ ‡å‡†çœä»½": None, "æ ‡å‡†åŸå¸‚": None,
                        "ç½®ä¿¡åº¦": res.get('confidence', 0.0),
                        "åŒ¹é…åŸå› ": res.get('reason', 'AIæœªæ‰¾åˆ°')
                    }
                    
                    if matched_id and str(matched_id) in cand_map:
                        m_row = cand_map[str(matched_id)]
                        out_row.update({
                            "åŒ¹é…çŠ¶æ€": "AIåŒ¹é…",
                            "æ ‡å‡†ç¼–ç ": m_row[MASTER_COL_CODE],
                            "æ ‡å‡†åç§°": m_row[MASTER_COL_NAME],
                            "æ ‡å‡†çœä»½": m_row[MASTER_COL_PROV],
                            "æ ‡å‡†åŸå¸‚": m_row[MASTER_COL_CITY]
                        })
                    parsed_results.append(out_row)
                return parsed_results
            
        except Exception as e:
            last_error = str(e)
            if "429" in last_error or "503" in last_error:
                continue
            else:
                break
                
    # å¦‚æœå…¨éƒ¨å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœï¼Œå¹¶åœ¨å¤–éƒ¨æ ‡è®°é”™è¯¯
    return []

def process_batch_job(batch_data, df_master, col_map, clients):
    """
    Worker å‡½æ•°ï¼šå¤„ç†ä¸€ä¸ª Batch
    batch_data: (batch_key, dataframe_slice)
    """
    (prov, city), df_batch = batch_data
    
    # 1. è·å–è¯¥åœ°åŒºçš„å€™é€‰æ±  (500æ¡ä»¥å†…)
    candidates, source_info = get_batch_candidates(df_master, df_batch, col_map, limit=CANDIDATE_LIMIT)
    
    results = []
    
    # å¦‚æœå€™é€‰æ± ä¸ºç©ºï¼Œç›´æ¥å…¨éƒ¨æ ‡è®°å¤±è´¥
    if candidates.empty:
        for idx, _ in df_batch.iterrows():
            results.append({
                "idx": idx,
                "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                "åŒ¹é…åŸå› ": f"æ ‡å‡†åº“ä¸­æ— [{prov}-{city}]æ•°æ®",
                "ç½®ä¿¡åº¦": 0.0
            })
        return results

    # 2. è°ƒç”¨ AI
    ai_results = call_ai_batch_process(clients, df_batch, candidates, col_map, f"{prov}_{city}")
    
    # 3. åˆå¹¶ç»“æœï¼ˆé˜²æ­¢ AI æ¼æ‰æŸäº›æ¡ç›®ï¼‰
    # åˆ›å»ºä¸€ä¸ª map æ–¹ä¾¿æŸ¥æ‰¾
    ai_res_map = {r['idx']: r for r in ai_results}
    
    final_results = []
    for idx, _ in df_batch.iterrows():
        if idx in ai_res_map:
            final_results.append(ai_res_map[idx])
        else:
            # AI æ¼æ‰äº†è¿™æ¡ï¼ˆæå°‘æƒ…å†µï¼‰ï¼Œæ ‡è®°ä¸ºå¤±è´¥
            final_results.append({
                "idx": idx,
                "åŒ¹é…çŠ¶æ€": "AIæœªåŒ¹é…",
                "åŒ¹é…åŸå› ": "AIå“åº”é—æ¼",
                "ç½®ä¿¡åº¦": 0.0
            })
            
    return final_results

# ================= 5. UI ä¸ ä¸»é€»è¾‘ =================

inject_custom_css()
clients = get_clients()

if "df_result" not in st.session_state: st.session_state.df_result = None
if "mapping_confirmed" not in st.session_state: st.session_state.mapping_confirmed = False
if "processing" not in st.session_state: st.session_state.processing = False
if "stop_signal" not in st.session_state: st.session_state.stop_signal = False
if "col_map" not in st.session_state: st.session_state.col_map = {}
if "df_master" not in st.session_state: st.session_state.df_master = load_cached_master()

with st.sidebar:
    st.image("https://cdn-icons-png.flaticon.com/512/3063/3063823.png", width=60)
    st.title("ChatMDM")
    st.caption("Region-Batch Edition")
    st.markdown("---")
    
    if st.session_state.df_master is not None:
        st.success(f"âœ… æ ‡å‡†åº“: {len(st.session_state.df_master):,} æ¡")
        if st.button("ğŸ—‘ï¸ é‡æ–°ä¸Šä¼ æ ‡å‡†åº“"):
            clear_master_cache()
            st.session_state.df_master = None
            st.rerun()
    else:
        master_file = st.file_uploader("ä¸Šä¼ æ ‡å‡†åº“ (xlsx/csv)", type=["xlsx", "csv"])
        if master_file:
            df_proc, msg = process_master_data(master_file)
            if df_proc is not None:
                st.session_state.df_master = df_proc
                save_master_cache(df_proc)
                st.rerun()
            else: st.error(msg)
    
    st.divider()
    if st.button("ğŸ”„ é‡ç½®æ‰€æœ‰ä»»åŠ¡"):
        bak = st.session_state.df_master
        st.session_state.clear()
        st.session_state.df_master = bak
        st.rerun()
        
    if st.session_state.df_result is not None:
        st.divider()
        df_exp = st.session_state.df_result
        csv = df_exp.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ", csv, "mdm_result.csv", "text/csv", type="primary")

st.title("ğŸ¥ åŒ»ç–—ä¸»æ•°æ®æ¸…æ´— (åœ°åŒºèšåˆ + Batchå¹¶å‘)")

if not clients: st.error("âŒ æœªæ£€æµ‹åˆ° API Keyï¼Œè¯·åœ¨ Secrets ä¸­é…ç½® GENAI_API_KEY")
if st.session_state.df_master is None: st.info("è¯·å…ˆä¸Šä¼ æ ‡å‡†åº“"); st.stop()

# 1. ä¸Šä¼ å¾…æ´—æ•°æ®
if st.session_state.df_result is None:
    target_file = st.file_uploader("ä¸Šä¼ å¾…æ¸…æ´—æ•°æ®", type=["xlsx", "csv"])
    if target_file:
        if target_file.name.endswith('.csv'): df_t = pd.read_csv(target_file)
        else: df_t = pd.read_excel(target_file)
        df_t = df_t.astype(str)
        # åˆå§‹åŒ–ç»“æœåˆ—
        for c in ['åŒ¹é…çŠ¶æ€', 'æ ‡å‡†ç¼–ç ', 'æ ‡å‡†åç§°', 'æ ‡å‡†çœä»½', 'æ ‡å‡†åŸå¸‚', 'åŒ¹é…åŸå› ']: df_t[c] = None
        df_t['åŒ¹é…çŠ¶æ€'] = 'å¾…å¤„ç†'
        df_t['ç½®ä¿¡åº¦'] = 0.0
        st.session_state.df_result = df_t
        st.rerun()

# 2. æ˜ å°„å­—æ®µ
elif not st.session_state.mapping_confirmed:
    cols = st.session_state.df_result.columns.tolist()
    c1, c2, c3 = st.columns(3)
    t_name = c1.selectbox("åç§°åˆ—", cols)
    t_prov = c2.selectbox("çœä»½åˆ—", cols) # åœ°åŒºåˆ†ç»„å¿…é¡»è¦æœ‰çœå¸‚
    t_city = c3.selectbox("åŸå¸‚åˆ—", cols)
    
    if st.button("ğŸš€ å¼€å§‹æ¸…æ´—é…ç½®"):
        st.session_state.col_map = {"target_name": t_name, "target_province": t_prov, "target_city": t_city}
        st.session_state.mapping_confirmed = True
        st.rerun()

# 3. æ‰§è¡Œæ§åˆ¶å°
else:
    df_curr = st.session_state.df_result
    col_map = st.session_state.col_map
    
    # ç»Ÿè®¡é¢æ¿
    done = len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] != 'å¾…å¤„ç†'])
    c1, c2, c3, c4 = st.columns(4)
    render_metric_card("æ€»è¿›åº¦", f"{done}/{len(df_curr)}")
    render_metric_card("å…¨å­—åŒ¹é…", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'å…¨å­—åŒ¹é…']))
    render_metric_card("AI å‘½ä¸­", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'AIåŒ¹é…']))
    render_metric_card("æœªå‘½ä¸­", len(df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'AIæœªåŒ¹é…']))
    
    st.divider()
    
    col_act, col_view = st.columns([1, 4])
    
    with col_act:
        # A. Hash åŒ¹é… (é¢„å¤„ç†)
        if st.button("âš¡ Step 1: ç²¾ç¡®åŒ¹é…", use_container_width=True, disabled=st.session_state.processing):
            with st.spinner("Hash ç¢°æ’ä¸­..."):
                # === ä¿®å¤å¼€å§‹ ===
                # 1. æå–å¿…è¦çš„åˆ—ï¼Œå¹¶å»é™¤é‡å¤çš„â€œåŒ»é™¢åç§°â€
                # keep='first' è¡¨ç¤ºå¦‚æœåå­—é‡å¤ï¼Œä¿ç•™ç¬¬ä¸€æ¡å‡ºç°çš„ï¼ˆé€šå¸¸æ ‡å‡†åº“é‡å¤é¡¹ä¹Ÿæ˜¯æŒ‡å‘åŒä¸€ä¸ªç¼–ç ï¼‰
                master_deduped = st.session_state.df_master.drop_duplicates(subset=[MASTER_COL_NAME], keep='first')
                
                # 2. å®‰å…¨åœ°è½¬æ¢ä¸ºå­—å…¸
                master_dict = master_deduped.set_index(MASTER_COL_NAME).to_dict('index')
                # === ä¿®å¤ç»“æŸ ===

                mask = (df_curr['åŒ¹é…çŠ¶æ€'] == 'å¾…å¤„ç†') & (df_curr[col_map['target_name']].isin(master_dict))
                if mask.any():
                    # å¿«é€Ÿå›å¡«
                    def _fill(n): return master_dict.get(n, {})
                    matches = df_curr.loc[mask, col_map['target_name']].apply(_fill)
                    df_curr.loc[mask, 'æ ‡å‡†ç¼–ç '] = matches.apply(lambda x: x.get(MASTER_COL_CODE))
                    df_curr.loc[mask, 'æ ‡å‡†åç§°'] = df_curr.loc[mask, col_map['target_name']]
                    df_curr.loc[mask, 'æ ‡å‡†çœä»½'] = matches.apply(lambda x: x.get(MASTER_COL_PROV))
                    df_curr.loc[mask, 'æ ‡å‡†åŸå¸‚'] = matches.apply(lambda x: x.get(MASTER_COL_CITY))
                    df_curr.loc[mask, 'åŒ¹é…çŠ¶æ€'] = 'å…¨å­—åŒ¹é…'
                    df_curr.loc[mask, 'ç½®ä¿¡åº¦'] = 1.0
                    st.session_state.df_result = df_curr
                    st.rerun()
                else:
                    st.warning("æ²¡æœ‰å‘ç°å…¨å­—åŒ¹é…çš„é¡¹ç›®ï¼Œè¯·ç›´æ¥ä½¿ç”¨ AI åŒ¹é…ã€‚")
        
        # B. AI Batch åŒ¹é…
        if not st.session_state.processing:
            if st.button("ğŸ§  Step 2: AI èšåˆåŒ¹é…", type="primary", use_container_width=True):
                st.session_state.processing = True
                st.session_state.stop_signal = False
                st.rerun()
        else:
            if st.button("ğŸ›‘ æš‚åœä»»åŠ¡", type="secondary", use_container_width=True):
                st.session_state.stop_signal = True
                st.session_state.processing = False
                st.rerun()
                
    with col_view:
        p_bar = st.progress(0)
        status_txt = st.empty()
        table_ph = st.empty()
        table_ph.dataframe(df_curr.head(100), height=300, use_container_width=True)
        
        if st.session_state.processing:
            # 1. ç­›é€‰å¾…å¤„ç†æ•°æ®
            pending_df = df_curr[df_curr['åŒ¹é…çŠ¶æ€'] == 'å¾…å¤„ç†'].copy()
            
            if pending_df.empty:
                st.session_state.processing = False
                st.success("æ‰€æœ‰æ•°æ®å·²å¤„ç†å®Œæ¯•ï¼")
                st.rerun()
            
            # 2. ç”Ÿæˆä»»åŠ¡æ‰¹æ¬¡ (Batch Generation)
            status_txt.text("æ­£åœ¨æŒ‰åœ°åŒºèšåˆåˆ†ç»„...")
            batches = []
            
            # æŒ‰çœå¸‚åˆ†ç»„
            grouped = pending_df.groupby([col_map['target_province'], col_map['target_city']])
            
            for (prov, city), group_df in grouped:
                # ç»„å†…å†åˆ‡ç‰‡ï¼Œæ¯ BATCH_SIZE æ¡ä¸€ç»„
                total_in_group = len(group_df)
                for i in range(0, total_in_group, BATCH_SIZE):
                    batch_slice = group_df.iloc[i : i + BATCH_SIZE]
                    batches.append(((prov, city), batch_slice))
            
            total_batches = len(batches)
            status_txt.text(f"ç”Ÿæˆ {total_batches} ä¸ªæ‰¹æ¬¡ä»»åŠ¡ (æ¯æ‰¹çº¦ {BATCH_SIZE} æ¡)...")
            
            # 3. å¹¶å‘æ‰§è¡Œ
            # ç”±äºæ˜¯ Batch å¤„ç†ï¼Œæ¯ä¸ª Batch è€—æ—¶è¾ƒé•¿ï¼ˆI/Oå¤šï¼‰ï¼ŒKeyåˆ©ç”¨ç‡é«˜
            MAX_WORKERS = min(len(clients) * 2, 6) # æ§åˆ¶åœ¨åˆç†èŒƒå›´
            
            completed_batches = 0
            results_buffer = []
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_map = {
                    executor.submit(process_batch_job, b, st.session_state.df_master, col_map, clients): i 
                    for i, b in enumerate(batches)
                }
                
                start_ts = time.time()
                
                for future in concurrent.futures.as_completed(future_map):
                    if st.session_state.stop_signal: break
                    
                    try:
                        batch_res = future.result()
                        results_buffer.extend(batch_res) # æ”¶é›†ç»“æœ
                    except Exception as e:
                        print(e)
                    
                    completed_batches += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    p_val = completed_batches / total_batches
                    p_bar.progress(p_val)
                    
                    elapsed = time.time() - start_ts
                    speed = (completed_batches * BATCH_SIZE) / elapsed if elapsed > 0 else 0
                    status_txt.markdown(f"**AIå¤„ç†ä¸­...** | åœ°åŒºç»„å¤„ç†è¿›åº¦: {completed_batches}/{total_batches} | ä¼°ç®—é€Ÿåº¦: {speed:.1f} æ¡/ç§’")
                    
                    # æ‰¹é‡åˆ·æ–°UI (æ¯å¤„ç†å®Œ 2 ä¸ª Batch åˆ·æ–°ä¸€æ¬¡)
                    if len(results_buffer) >= BATCH_SIZE * 2:
                        for res in results_buffer:
                            idx = res['idx']
                            for k, v in res.items():
                                if k != 'idx': df_curr.at[idx, k] = v
                        results_buffer = []
                        table_ph.dataframe(df_curr.head(50), height=300, use_container_width=True)
            
            # å¤„ç†å‰©ä½™
            if results_buffer:
                for res in results_buffer:
                    idx = res['idx']
                    for k, v in res.items():
                        if k != 'idx': df_curr.at[idx, k] = v
            
            st.session_state.df_result = df_curr
            st.session_state.processing = False
            st.rerun()





